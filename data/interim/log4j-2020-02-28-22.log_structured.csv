LineId,Date,Time,Level,Component,Content,EventId,EventTemplate,ParameterList
1,20/02/28,22:47:30,INFO,StaticConf$,DB_HOME: /databricks,18bf7240,DB_HOME: /databricks,[]
2,20/02/28,22:47:30,INFO,DriverDaemon$,========== driver starting up ==========,1c257a9c,========== driver starting up ==========,[]
3,20/02/28,22:47:30,INFO,DriverDaemon$,Java: Private Build 1.8.0_232,4655b4ae,Java: Private Build 1.8.0_232,[]
4,20/02/28,22:47:30,INFO,DriverDaemon$,OS: Linux/amd64 4.4.0-1102-aws,028867b6,OS: Linux/amd64 4.4.0-1102-aws,[]
5,20/02/28,22:47:30,INFO,DriverDaemon$,CWD: /databricks/driver,d74d6910,CWD: /databricks/driver,[]
6,20/02/28,22:47:30,INFO,DriverDaemon$,"Mem: Max: 4.8G loaded GCs: PS Scavenge, PS MarkSweep",ecab9817,"Mem: Max: 4.8G loaded GCs: PS Scavenge, PS MarkSweep",[]
7,20/02/28,22:47:30,INFO,DriverDaemon$,Logging multibyte characters: âœ“,ad84d2dd,Logging multibyte characters: âœ“,[]
8,20/02/28,22:47:30,INFO,DriverDaemon$,'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender,4690b9f6,'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender,[]
9,20/02/28,22:47:30,INFO,DriverDaemon$,'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender,38f76352,<*> appender in root logger: class <*>,[]
10,20/02/28,22:47:30,INFO,DriverDaemon$,'null' appender in root logger: class com.databricks.logging.RequestTracker,38f76352,<*> appender in root logger: class <*>,[]
11,20/02/28,22:47:30,INFO,DriverDaemon$,== Modules:,4fce4bb4,== Modules:,[]
12,20/02/28,22:47:31,INFO,DriverDaemon$,Starting prometheus metrics log export timer,e662b3c6,Starting prometheus metrics log export timer,[]
13,20/02/28,22:47:31,INFO,DriverDaemon$,Universe Git Hash: 687e04cafbfe88e512c6aea9e4f53a2c4e1d5689,8cd83268,Universe Git Hash: 687e04cafbfe88e512c6aea9e4f53a2c4e1d5689,[]
14,20/02/28,22:47:31,INFO,DriverDaemon$,Spark Git Hash: 428199fa1bdaea59715ade6e7d4d6bb5a11c388d,20b49405,Spark Git Hash: 428199fa1bdaea59715ade6e7d4d6bb5a11c388d,[]
15,20/02/28,22:47:31,WARN,RunHelpers$,"Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)",712bd1a0,"Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)",[]
16,20/02/28,22:47:31,INFO,DatabricksILoop$,Creating throwaway interpreter,b14b4ca1,Creating throwaway interpreter,[]
17,20/02/28,22:47:31,INFO,SparkConfUtils$,Customize spark config according to file /tmp/custom-spark.conf,e28cf8b0,Customize spark config according to file /tmp/custom-spark.conf,[]
18,20/02/28,22:47:31,INFO,MetastoreMonitor$,"Internal internal metastore configured (config=DbMetastoreConfig{host=devtierprod1-db.caj77bnxuhme.us-west-2.rds.amazonaws.com, port=3306, dbName=organization823618267676840, user=YbAhhCtN6Tgu0BzS})",365b761d,"Internal internal metastore configured (config=DbMetastoreConfig{host=devtierprod1-db.caj77bnxuhme.us-west-2.rds.amazonaws.com, port=3306, dbName=organization823618267676840, user=YbAhhCtN6Tgu0BzS})",[]
19,20/02/28,22:47:32,INFO,HikariDataSource,metastore-monitor - Starting...,927bc97a,metastore-monitor - Starting...,[]
20,20/02/28,22:47:32,INFO,HikariDataSource,metastore-monitor - Start completed.,5b7e5d94,metastore-monitor - <*> completed.,[]
21,20/02/28,22:47:32,INFO,DriverCorral,Creating the driver context,d3a0de76,Creating the driver context,[]
22,20/02/28,22:47:32,INFO,DatabricksILoop$,Class Server Dir: /local_disk0/tmp/repl/spark-3281211262966016233-8fe57c53-b822-4434-871c-c263ce4408f3,1a09080b,Class Server Dir: /local_disk0/tmp/repl/spark-3281211262966016233-8fe57c53-b822-4434-871c-c263ce4408f3,[]
23,20/02/28,22:47:32,INFO,SparkConfUtils$,Customize spark config according to file /tmp/custom-spark.conf,e28cf8b0,Customize spark config according to file /tmp/custom-spark.conf,[]
24,20/02/28,22:47:32,WARN,SparkConf,The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.,d62e22a5,The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.,[]
25,20/02/28,22:47:32,INFO,HikariDataSource,metastore-monitor - Shutdown initiated...,79466fcb,metastore-monitor - Shutdown initiated...,[]
26,20/02/28,22:47:32,INFO,HikariDataSource,metastore-monitor - Shutdown completed.,5b7e5d94,metastore-monitor - <*> completed.,[]
27,20/02/28,22:47:32,INFO,MetastoreMonitor,Metastore healthcheck successful (connection duration = 941 milliseconds),a68c364d,Metastore healthcheck successful (connection duration = <*> milliseconds),[]
28,20/02/28,22:47:32,INFO,SparkContext,Running Spark version 2.4.4,fc4b7056,Running Spark version 2.4.4,[]
29,20/02/28,22:47:33,WARN,SparkConf,"Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).",d123ccb4,"Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).",[]
30,20/02/28,22:47:33,INFO,SparkContext,Submitted application: Databricks Shell,a7c31293,Submitted application: Databricks Shell,[]
31,20/02/28,22:47:33,INFO,SecurityManager,Changing view acls to: root,b810e215,Changing <*> acls to: root,[]
32,20/02/28,22:47:33,INFO,SecurityManager,Changing modify acls to: root,b810e215,Changing <*> acls to: root,[]
33,20/02/28,22:47:33,INFO,SecurityManager,Changing view acls groups to:,66c1cc9e,Changing <*> acls groups to:,[]
34,20/02/28,22:47:33,INFO,SecurityManager,Changing modify acls groups to:,66c1cc9e,Changing <*> acls groups to:,[]
35,20/02/28,22:47:33,INFO,SecurityManager,SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set(),f37a5454,SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(),[]
36,20/02/28,22:47:34,INFO,Utils,Successfully started service 'sparkDriver' on port 34230.,f5a68e94,Successfully started service <*> on port <*>,[]
37,20/02/28,22:47:34,INFO,SparkEnv,Registering MapOutputTracker,657cfe01,Registering MapOutputTracker,[]
38,20/02/28,22:47:34,INFO,SparkEnv,Registering BlockManagerMaster,d353d6f7,Registering BlockManagerMaster,[]
39,20/02/28,22:47:34,INFO,BlockManagerMasterEndpoint,Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information,add369a7,Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information,[]
40,20/02/28,22:47:34,INFO,BlockManagerMasterEndpoint,BlockManagerMasterEndpoint up,f8f0a311,BlockManagerMasterEndpoint up,[]
41,20/02/28,22:47:34,INFO,DiskBlockManager,Created local directory at /local_disk0/blockmgr-8d21e9b3-3221-4782-821f-fca3197f1480,2e39369f,Created local directory at /local_disk0/blockmgr-8d21e9b3-3221-4782-821f-fca3197f1480,[]
42,20/02/28,22:47:34,INFO,MemoryStore,MemoryStore started with capacity 2.5 GB,fafeec11,MemoryStore started with capacity 2.5 GB,[]
43,20/02/28,22:47:34,INFO,SparkEnv,Registering OutputCommitCoordinator,dc5b83ae,Registering OutputCommitCoordinator,[]
44,20/02/28,22:47:34,INFO,SparkContext,Spark configuration:,6d9d4370,Spark configuration:,[]
45,20/02/28,22:47:34,WARN,MetricsSystem,Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.,1d120ff3,Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.,[]
46,20/02/28,22:47:34,INFO,log,Logging initialized @7531ms,1424647a,Logging initialized @7531ms,[]
47,20/02/28,22:47:34,INFO,Server,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",3a1b91d0,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",[]
48,20/02/28,22:47:34,INFO,Server,Started @7697ms,c66d4456,Started @7697ms,[]
49,20/02/28,22:47:34,INFO,AbstractConnector,"Started ServerConnector@67e25252{HTTP/1.1,[http/1.1]}{10.172.228.50:42605}",4df8c34f,"Started ServerConnector@67e25252{HTTP/1.1,[http/1.1]}{10.172.228.50:42605}",[]
50,20/02/28,22:47:34,INFO,Utils,Successfully started service 'SparkUI' on port 42605.,f5a68e94,Successfully started service <*> on port <*>,[]
51,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@55fee662{/jobs,null,AVAILABLE,@Spark}",00329def,"Started o.e.j.s.ServletContextHandler@55fee662{/jobs,null,AVAILABLE,@Spark}",[]
52,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@309cedb6{/jobs/json,null,AVAILABLE,@Spark}",479d08b6,"Started o.e.j.s.ServletContextHandler@309cedb6{/jobs/json,null,AVAILABLE,@Spark}",[]
53,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@3b95a6db{/jobs/job,null,AVAILABLE,@Spark}",45c3a018,"Started o.e.j.s.ServletContextHandler@3b95a6db{/jobs/job,null,AVAILABLE,@Spark}",[]
54,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@2c9a6717{/jobs/job/json,null,AVAILABLE,@Spark}",2079e744,"Started o.e.j.s.ServletContextHandler@2c9a6717{/jobs/job/json,null,AVAILABLE,@Spark}",[]
55,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@7b3cde6f{/stages,null,AVAILABLE,@Spark}",d28dac24,"Started o.e.j.s.ServletContextHandler@7b3cde6f{/stages,null,AVAILABLE,@Spark}",[]
56,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@6d091cad{/stages/json,null,AVAILABLE,@Spark}",eac877da,"Started o.e.j.s.ServletContextHandler@6d091cad{/stages/json,null,AVAILABLE,@Spark}",[]
57,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@7c663eaf{/stages/stage,null,AVAILABLE,@Spark}",c8528545,"Started o.e.j.s.ServletContextHandler@7c663eaf{/stages/stage,null,AVAILABLE,@Spark}",[]
58,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@3ba0ae41{/stages/stage/json,null,AVAILABLE,@Spark}",37c56155,"Started o.e.j.s.ServletContextHandler@3ba0ae41{/stages/stage/json,null,AVAILABLE,@Spark}",[]
59,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@76fe6cdc{/stages/pool,null,AVAILABLE,@Spark}",abad6f2c,"Started o.e.j.s.ServletContextHandler@76fe6cdc{/stages/pool,null,AVAILABLE,@Spark}",[]
60,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@2ffb3aec{/stages/pool/json,null,AVAILABLE,@Spark}",03fb7f98,"Started o.e.j.s.ServletContextHandler@2ffb3aec{/stages/pool/json,null,AVAILABLE,@Spark}",[]
61,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@786ff1cb{/storage,null,AVAILABLE,@Spark}",f7c61d5b,"Started o.e.j.s.ServletContextHandler@786ff1cb{/storage,null,AVAILABLE,@Spark}",[]
62,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@46039a21{/storage/json,null,AVAILABLE,@Spark}",768a2155,"Started o.e.j.s.ServletContextHandler@46039a21{/storage/json,null,AVAILABLE,@Spark}",[]
63,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@431e86b1{/storage/rdd,null,AVAILABLE,@Spark}",c69cc109,"Started o.e.j.s.ServletContextHandler@431e86b1{/storage/rdd,null,AVAILABLE,@Spark}",[]
64,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@35c4e864{/storage/rdd/json,null,AVAILABLE,@Spark}",7ebb864e,"Started o.e.j.s.ServletContextHandler@35c4e864{/storage/rdd/json,null,AVAILABLE,@Spark}",[]
65,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@32a2a6be{/environment,null,AVAILABLE,@Spark}",ac704288,"Started o.e.j.s.ServletContextHandler@32a2a6be{/environment,null,AVAILABLE,@Spark}",[]
66,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@682af059{/environment/json,null,AVAILABLE,@Spark}",d9ea710d,"Started o.e.j.s.ServletContextHandler@682af059{/environment/json,null,AVAILABLE,@Spark}",[]
67,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@5f36c8e3{/executors,null,AVAILABLE,@Spark}",aab29aa7,"Started o.e.j.s.ServletContextHandler@5f36c8e3{/executors,null,AVAILABLE,@Spark}",[]
68,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@4da39ca9{/executors/json,null,AVAILABLE,@Spark}",aee761c3,"Started o.e.j.s.ServletContextHandler@4da39ca9{/executors/json,null,AVAILABLE,@Spark}",[]
69,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@6a9344f5{/executors/threadDump,null,AVAILABLE,@Spark}",0ca7aef7,"Started o.e.j.s.ServletContextHandler@6a9344f5{/executors/threadDump,null,AVAILABLE,@Spark}",[]
70,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@5584d9c6{/executors/threadDump/json,null,AVAILABLE,@Spark}",0749d11a,"Started o.e.j.s.ServletContextHandler@5584d9c6{/executors/threadDump/json,null,AVAILABLE,@Spark}",[]
71,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@3c9c6245{/executors/heapHistogram,null,AVAILABLE,@Spark}",7455dc22,"Started o.e.j.s.ServletContextHandler@3c9c6245{/executors/heapHistogram,null,AVAILABLE,@Spark}",[]
72,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@6d0be7ab{/executors/heapHistogram/json,null,AVAILABLE,@Spark}",9256fd10,"Started o.e.j.s.ServletContextHandler@6d0be7ab{/executors/heapHistogram/json,null,AVAILABLE,@Spark}",[]
73,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@1d4fb213{/static,null,AVAILABLE,@Spark}",454c99d7,"Started o.e.j.s.ServletContextHandler@1d4fb213{/static,null,AVAILABLE,@Spark}",[]
74,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@1668919e{/,null,AVAILABLE,@Spark}",e29939b0,"Started o.e.j.s.ServletContextHandler@1668919e{/,null,AVAILABLE,@Spark}",[]
75,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@63300c4b{/api,null,AVAILABLE,@Spark}",38c81fe7,"Started o.e.j.s.ServletContextHandler@63300c4b{/api,null,AVAILABLE,@Spark}",[]
76,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@b67cc70{/jobs/job/kill,null,AVAILABLE,@Spark}",3f089cdf,"Started o.e.j.s.ServletContextHandler@b67cc70{/jobs/job/kill,null,AVAILABLE,@Spark}",[]
77,20/02/28,22:47:34,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@45c9b3{/stages/stage/kill,null,AVAILABLE,@Spark}",2f4d9d25,"Started o.e.j.s.ServletContextHandler@45c9b3{/stages/stage/kill,null,AVAILABLE,@Spark}",[]
78,20/02/28,22:47:34,INFO,SparkUI,"Bound SparkUI to 10.172.228.50, and started at http://10.172.228.50:42605",c4ab676f,"Bound SparkUI to 10.172.228.50, and started at http://10.172.228.50:42605",[]
79,20/02/28,22:47:34,WARN,FairSchedulableBuilder,"Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.",26d69558,"Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.",[]
80,20/02/28,22:47:34,INFO,FairSchedulableBuilder,"Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1",d7be2f12,"Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1",[]
81,20/02/28,22:47:34,INFO,Executor,Starting executor ID driver on host localhost,9b7c20e2,Starting executor ID driver on host localhost,[]
82,20/02/28,22:47:35,INFO,Executor,Using REPL class URI: spark://10.172.228.50:34230/classes,bae0caf2,Using REPL class URI: spark://10.172.228.50:34230/classes,[]
83,20/02/28,22:47:35,INFO,TaskSchedulerImpl,Task preemption enabled.,958d481c,Task preemption enabled.,[]
84,20/02/28,22:47:35,INFO,Utils,Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33830.,f5a68e94,Successfully started service <*> on port <*>,[]
85,20/02/28,22:47:35,INFO,NettyBlockTransferService,Server created on 10.172.228.50:33830,2e344772,Server created on 10.172.228.50:33830,[]
86,20/02/28,22:47:35,INFO,BlockManager,Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy,aafe1779,Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy,[]
87,20/02/28,22:47:35,INFO,BlockManagerMaster,"Registering BlockManager BlockManagerId(driver, 10.172.228.50, 33830, None)",b26d8498,"Registering BlockManager BlockManagerId(driver, 10.172.228.50, 33830, None)",[]
88,20/02/28,22:47:35,INFO,BlockManagerMasterEndpoint,"Registering block manager 10.172.228.50:33830 with 2.5 GB RAM, BlockManagerId(driver, 10.172.228.50, 33830, None)",754814e0,"Registering block manager 10.172.228.50:33830 with 2.5 GB RAM, BlockManagerId(driver, 10.172.228.50, 33830, None)",[]
89,20/02/28,22:47:35,INFO,BlockManagerMaster,"Registered BlockManager BlockManagerId(driver, 10.172.228.50, 33830, None)",967da50a,"Registered BlockManager BlockManagerId(driver, 10.172.228.50, 33830, None)",[]
90,20/02/28,22:47:35,INFO,BlockManager,external shuffle service port = 4048,8247b6ce,external shuffle service port = 4048,[]
91,20/02/28,22:47:35,INFO,BlockManager,"Initialized BlockManager: BlockManagerId(driver, 10.172.228.50, 33830, None)",8adcd294,"Initialized BlockManager: BlockManagerId(driver, 10.172.228.50, 33830, None)",[]
92,20/02/28,22:47:35,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@1039bfc4{/metrics/json,null,AVAILABLE,@Spark}",c6cb6c38,"Started o.e.j.s.ServletContextHandler@1039bfc4{/metrics/json,null,AVAILABLE,@Spark}",[]
93,20/02/28,22:47:35,INFO,DBCEventLoggingListener,Initializing DBCEventLoggingListener,4e62be17,Initializing DBCEventLoggingListener,[]
94,20/02/28,22:47:35,INFO,DBCEventLoggingListener,Logging events to eventlogs/3281211262966016233/eventlog,08411049,Logging events to eventlogs/3281211262966016233/eventlog,[]
95,20/02/28,22:47:35,INFO,SparkContext,Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener,65e66212,Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener,[]
96,20/02/28,22:47:35,INFO,SparkContext,Loading Spark Service RPC Server,fe11ae85,Loading Spark Service RPC Server,[]
97,20/02/28,22:47:36,INFO,SparkServiceRPCServer,Starting Spark Service RPC Server,2f38be19,Starting Spark Service RPC Server,[]
98,20/02/28,22:47:36,INFO,Server,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",3a1b91d0,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",[]
99,20/02/28,22:47:36,INFO,AbstractConnector,"Started ServerConnector@543d5863{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}",bb3959d0,"Started ServerConnector@543d5863{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}",[]
100,20/02/28,22:47:36,INFO,Server,Started @9183ms,8286b4f4,Started @9183ms,[]
101,20/02/28,22:47:36,INFO,DatabricksILoop$,Successfully registered spark metrics in Prometheus registry,4558595b,Successfully registered spark metrics in Prometheus registry,[]
102,20/02/28,22:47:36,INFO,DatabricksILoop$,Successfully initialized SparkContext,804edc76,Successfully initialized SparkContext,[]
103,20/02/28,22:47:36,INFO,DBFS,Initialized DBFS with DBFSV1 as the delegate.,2eaf7cc5,Initialized DBFS with DBFSV1 as the delegate.,[]
104,20/02/28,22:47:36,WARN,JettyClientConf$,Could not load DynamicJettyClientConf,6ade2511,Could not load DynamicJettyClientConf,[]
105,20/02/28,22:47:36,INFO,JettyClient$,"Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DBFSV1), idleTimeout=2 hours, useBlockingConnect: true",00f8393c,"Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, <*> idleTimeout=2 hours, useBlockingConnect: true",[]
106,20/02/28,22:47:37,INFO,SharedState,Scheduler stats enabled.,c4ab4fbf,Scheduler stats enabled.,[]
107,20/02/28,22:47:37,INFO,SharedState,loading hive config file: file:/databricks/hive/conf/hive-site.xml,af7e1aed,loading hive config file: file:/databricks/hive/conf/hive-site.xml,[]
108,20/02/28,22:47:37,INFO,SharedState,Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').,976cbf3b,Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').,[]
109,20/02/28,22:47:37,INFO,SharedState,Warehouse path is '/user/hive/warehouse'.,2295f829,Warehouse path is '/user/hive/warehouse'.,[]
110,20/02/28,22:47:37,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@5b066c33{/SQL,null,AVAILABLE,@Spark}",a404be4e,"Started o.e.j.s.ServletContextHandler@5b066c33{/SQL,null,AVAILABLE,@Spark}",[]
111,20/02/28,22:47:37,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@62ea8931{/SQL/json,null,AVAILABLE,@Spark}",74b02e22,"Started o.e.j.s.ServletContextHandler@62ea8931{/SQL/json,null,AVAILABLE,@Spark}",[]
112,20/02/28,22:47:37,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@71166348{/SQL/execution,null,AVAILABLE,@Spark}",bda8c3ce,"Started o.e.j.s.ServletContextHandler@71166348{/SQL/execution,null,AVAILABLE,@Spark}",[]
113,20/02/28,22:47:37,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@6d874695{/SQL/execution/json,null,AVAILABLE,@Spark}",5f900836,"Started o.e.j.s.ServletContextHandler@6d874695{/SQL/execution/json,null,AVAILABLE,@Spark}",[]
114,20/02/28,22:47:37,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@10ed037a{/static/sql,null,AVAILABLE,@Spark}",13f8d2c1,"Started o.e.j.s.ServletContextHandler@10ed037a{/static/sql,null,AVAILABLE,@Spark}",[]
115,20/02/28,22:47:37,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@150fc7a7{/storage/iocache,null,AVAILABLE,@Spark}",b1b51d81,"Started o.e.j.s.ServletContextHandler@150fc7a7{/storage/iocache,null,AVAILABLE,@Spark}",[]
116,20/02/28,22:47:37,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@55d8c2c4{/storage/iocache/json,null,AVAILABLE,@Spark}",c9058e7c,"Started o.e.j.s.ServletContextHandler@55d8c2c4{/storage/iocache/json,null,AVAILABLE,@Spark}",[]
117,20/02/28,22:47:37,INFO,DatabricksILoop$,Finished creating throwaway interpreter,6c28c815,Finished creating throwaway interpreter,[]
118,20/02/28,22:47:38,INFO,StateStoreCoordinatorRef,Registered StateStoreCoordinator endpoint,931b701e,Registered StateStoreCoordinator endpoint,[]
119,20/02/28,22:47:40,INFO,HiveUtils,"Initializing execution hive, version 1.2.1",916787de,"Initializing execution hive, version 1.2.1",[]
120,20/02/28,22:47:41,INFO,HiveMetaStore,0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore,c513a2c4,<*> Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore,[]
121,20/02/28,22:47:41,INFO,ObjectStore,"ObjectStore, initialize called",f97d2395,"ObjectStore, initialize called",[]
122,20/02/28,22:47:41,INFO,Persistence,Property hive.metastore.integral.jdo.pushdown unknown - will be ignored,187a75f5,Property <*> unknown - will be ignored,[]
123,20/02/28,22:47:41,INFO,Persistence,Property datanucleus.cache.level2 unknown - will be ignored,187a75f5,Property <*> unknown - will be ignored,[]
124,20/02/28,22:47:41,INFO,Persistence,Property datanucleus.schema.autoCreateAll unknown - will be ignored,187a75f5,Property <*> unknown - will be ignored,[]
125,20/02/28,22:47:44,INFO,ObjectStore,"Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""",b90681cc,"Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""",[]
126,20/02/28,22:47:46,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
127,20/02/28,22:47:46,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
128,20/02/28,22:47:47,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
129,20/02/28,22:47:47,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
130,20/02/28,22:47:47,INFO,MetaStoreDirectSql,"Using direct SQL, underlying DB is DERBY",e8fc0ba7,"Using direct SQL, underlying DB is DERBY",[]
131,20/02/28,22:47:47,INFO,ObjectStore,Initialized ObjectStore,fee37921,Initialized ObjectStore,[]
132,20/02/28,22:47:47,WARN,ObjectStore,Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0,571c9214,Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0,[]
133,20/02/28,22:47:48,WARN,ObjectStore,"Failed to get database default, returning NoSuchObjectException",c111d0d0,"Failed to get database default, returning NoSuchObjectException",[]
134,20/02/28,22:47:48,INFO,HiveMetaStore,Added admin role in metastore,36a330ca,Added <*> role in metastore,[]
135,20/02/28,22:47:48,INFO,HiveMetaStore,Added public role in metastore,36a330ca,Added <*> role in metastore,[]
136,20/02/28,22:47:48,INFO,HiveMetaStore,"No user is added in admin role, since config is empty",ba641907,"No user is added in admin role, since config is empty",[]
137,20/02/28,22:47:48,INFO,HiveMetaStore,0: get_all_databases,494bd030,0: get_all_databases,[]
138,20/02/28,22:47:48,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_all_databases,a44f6d49,ugi=root ip=unknown-ip-addr cmd=get_all_databases,[]
139,20/02/28,22:47:48,INFO,HiveMetaStore,0: get_functions: db=default pat=*,c2534b22,0: <*> db=default pat=*,[]
140,20/02/28,22:47:48,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*,a5ec3f8e,ugi=root ip=unknown-ip-addr <*> db=default pat=*,[]
141,20/02/28,22:47:48,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MResourceUri"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
142,20/02/28,22:47:48,INFO,SessionState,Created HDFS directory: /tmp/hive/root,7984d8f5,Created HDFS directory: <*>,[]
143,20/02/28,22:47:48,INFO,SessionState,Created local directory: /local_disk0/tmp/root,98458554,Created local directory: <*>,[]
144,20/02/28,22:47:48,INFO,SessionState,Created local directory: /local_disk0/tmp/efd228cc-d90c-43d8-a496-232484c2a51c_resources,98458554,Created local directory: <*>,[]
145,20/02/28,22:47:49,INFO,SessionState,Created HDFS directory: /tmp/hive/root/efd228cc-d90c-43d8-a496-232484c2a51c,7984d8f5,Created HDFS directory: <*>,[]
146,20/02/28,22:47:49,INFO,SessionState,Created local directory: /local_disk0/tmp/root/efd228cc-d90c-43d8-a496-232484c2a51c,98458554,Created local directory: <*>,[]
147,20/02/28,22:47:49,INFO,SessionState,Created HDFS directory: /tmp/hive/root/efd228cc-d90c-43d8-a496-232484c2a51c/_tmp_space.db,7984d8f5,Created HDFS directory: <*>,[]
148,20/02/28,22:47:49,INFO,HiveClientImpl,Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse,0337ff6b,Warehouse location for Hive client (version <*> is /user/hive/warehouse,[]
149,20/02/28,22:47:49,INFO,SessionManager,Operation log root directory is created: /local_disk0/tmp/root/operation_logs,d2718f89,Operation log root directory is created: /local_disk0/tmp/root/operation_logs,[]
150,20/02/28,22:47:49,INFO,SessionManager,HiveServer2: Background operation thread pool size: 100,c40036b9,HiveServer2: Background operation thread pool size: 100,[]
151,20/02/28,22:47:49,INFO,SessionManager,HiveServer2: Background operation thread wait queue size: 100,df8ad543,HiveServer2: Background operation thread wait queue size: 100,[]
152,20/02/28,22:47:49,INFO,SessionManager,HiveServer2: Background operation thread keepalive time: 10 seconds,924bc959,HiveServer2: Background operation thread keepalive time: 10 seconds,[]
153,20/02/28,22:47:49,INFO,AbstractService,Service:OperationManager is inited.,b2e82aff,Service:OperationManager is <*>,[]
154,20/02/28,22:47:49,INFO,AbstractService,Service:SessionManager is inited.,4ca35cf6,Service:SessionManager is <*>,[]
155,20/02/28,22:47:49,INFO,AbstractService,Service: CLIService is inited.,abd5fe09,Service: <*> is inited.,[]
156,20/02/28,22:47:49,INFO,AbstractService,Service:ThriftHttpCLIService is inited.,d000210a,Service:ThriftHttpCLIService is <*>,[]
157,20/02/28,22:47:49,INFO,AbstractService,Service: HiveServer2 is inited.,abd5fe09,Service: <*> is inited.,[]
158,20/02/28,22:47:49,INFO,AbstractService,Service:OperationManager is started.,b2e82aff,Service:OperationManager is <*>,[]
159,20/02/28,22:47:49,INFO,AbstractService,Service:SessionManager is started.,4ca35cf6,Service:SessionManager is <*>,[]
160,20/02/28,22:47:49,INFO,AbstractService,Service:CLIService is started.,36671eb4,Service:CLIService is started.,[]
161,20/02/28,22:47:49,INFO,ObjectStore,"ObjectStore, initialize called",f97d2395,"ObjectStore, initialize called",[]
162,20/02/28,22:47:49,INFO,Query,"Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing",af819f61,"Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing",[]
163,20/02/28,22:47:49,INFO,MetaStoreDirectSql,"Using direct SQL, underlying DB is DERBY",e8fc0ba7,"Using direct SQL, underlying DB is DERBY",[]
164,20/02/28,22:47:49,INFO,ObjectStore,Initialized ObjectStore,fee37921,Initialized ObjectStore,[]
165,20/02/28,22:47:49,INFO,HiveMetaStore,0: get_databases: default,ef63b8b7,0: <*> default,[]
166,20/02/28,22:47:49,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_databases: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
167,20/02/28,22:47:49,INFO,HiveMetaStore,0: Shutting down the object store...,a70e3d9b,0: Shutting down the object store...,[]
168,20/02/28,22:47:49,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...,84c58f83,ugi=root ip=unknown-ip-addr cmd=Shutting down the object store...,[]
169,20/02/28,22:47:49,INFO,HiveMetaStore,0: Metastore shutdown complete.,32cf53be,0: Metastore shutdown complete.,[]
170,20/02/28,22:47:49,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.,27334525,ugi=root ip=unknown-ip-addr cmd=Metastore shutdown complete.,[]
171,20/02/28,22:47:49,INFO,AbstractService,Service:ThriftHttpCLIService is started.,d000210a,Service:ThriftHttpCLIService is <*>,[]
172,20/02/28,22:47:49,INFO,AbstractService,Service:HiveServer2 is started.,0f87fea4,Service:HiveServer2 is started.,[]
173,20/02/28,22:47:49,INFO,ThriftCLIService,"HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]",58a7872f,"HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]",[]
174,20/02/28,22:47:49,INFO,ThriftCLIService,"HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]",890e00e5,"HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]",[]
175,20/02/28,22:47:49,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@263f6e96{/sqlserver,null,AVAILABLE,@Spark}",72f9bd9d,"Started o.e.j.s.ServletContextHandler@263f6e96{/sqlserver,null,AVAILABLE,@Spark}",[]
176,20/02/28,22:47:49,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@49741e80{/sqlserver/json,null,AVAILABLE,@Spark}",20041d0d,"Started o.e.j.s.ServletContextHandler@49741e80{/sqlserver/json,null,AVAILABLE,@Spark}",[]
177,20/02/28,22:47:49,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@dd3e1e3{/sqlserver/session,null,AVAILABLE,@Spark}",6d591719,"Started o.e.j.s.ServletContextHandler@dd3e1e3{/sqlserver/session,null,AVAILABLE,@Spark}",[]
178,20/02/28,22:47:49,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@2686a801{/sqlserver/session/json,null,AVAILABLE,@Spark}",d2353d8b,"Started o.e.j.s.ServletContextHandler@2686a801{/sqlserver/session/json,null,AVAILABLE,@Spark}",[]
179,20/02/28,22:47:49,WARN,LibraryUtils$,Library file for validation /databricks/.python-env/packages_to_validate.json does not exist,1c71fa04,Library file for validation /databricks/.python-env/packages_to_validate.json does not exist,[]
180,20/02/28,22:47:49,INFO,DriverDaemon,Starting driver daemon...,12de4bbb,Starting driver daemon...,[]
181,20/02/28,22:47:49,INFO,SparkConfUtils$,Customize spark config according to file /tmp/custom-spark.conf,e28cf8b0,Customize spark config according to file /tmp/custom-spark.conf,[]
182,20/02/28,22:47:49,WARN,SparkConf,The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.,d62e22a5,The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.,[]
183,20/02/28,22:47:49,INFO,DriverDaemon$$anon$1,Message out thread ready,725b6ac7,Message out thread ready,[]
184,20/02/28,22:47:49,INFO,Server,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",3a1b91d0,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",[]
185,20/02/28,22:47:49,INFO,AbstractConnector,"Started ServerConnector@1a34f8e2{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}",685a90bf,"Started ServerConnector@1a34f8e2{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}",[]
186,20/02/28,22:47:49,INFO,Server,Started @22781ms,92f0caab,Started @22781ms,[]
187,20/02/28,22:47:49,INFO,DriverDaemon,Driver daemon started.,c7574caf,Driver daemon started.,[]
188,20/02/28,22:47:49,INFO,Server,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",3a1b91d0,"jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d",[]
189,20/02/28,22:47:49,WARN,SecurityHandler,"ServletContext@o.e.j.s.ServletContextHandler@77d00969{/,null,STARTING} has uncovered http methods for path: /*",75fd2b2c,"ServletContext@o.e.j.s.ServletContextHandler@77d00969{/,null,STARTING} has uncovered http methods for path: /*",[]
190,20/02/28,22:47:49,INFO,ContextHandler,"Started o.e.j.s.ServletContextHandler@77d00969{/,null,AVAILABLE}",be069e9a,"Started o.e.j.s.ServletContextHandler@77d00969{/,null,AVAILABLE}",[]
191,20/02/28,22:47:49,INFO,SslContextFactory,"x509=X509@2f07e871(1,h=[databrickscloud.com],w=[]) for SslContextFactory@711ca071(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)",20613e2e,"x509=X509@2f07e871(1,h=[databrickscloud.com],w=[]) for SslContextFactory@711ca071(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)",[]
192,20/02/28,22:47:49,INFO,AbstractConnector,"Started ServerConnector@3d2b92b5{SSL,[ssl, http/1.1]}{0.0.0.0:10000}",227c5dcf,"Started ServerConnector@3d2b92b5{SSL,[ssl, http/1.1]}{0.0.0.0:10000}",[]
193,20/02/28,22:47:49,INFO,Server,Started @22881ms,be7c0a22,Started @22881ms,[]
194,20/02/28,22:47:49,INFO,ThriftCLIService,Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads,7fd78aa1,Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads,[]
195,20/02/28,22:47:51,INFO,DriverCorral,Loading the root classloader,a90fba3b,Loading the root classloader,[]
196,20/02/28,22:47:51,INFO,DriverCorral,Starting sql repl ReplId-26113-21226-c29bd-0,6b45de39,Starting sql repl <*>,[]
197,20/02/28,22:47:51,WARN,SQLDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
198,20/02/28,22:47:51,INFO,DriverCorral,Starting sql repl ReplId-6fffc-24c49-c4ef9-6,6b45de39,Starting sql repl <*>,[]
199,20/02/28,22:47:51,WARN,SQLDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
200,20/02/28,22:47:51,INFO,SQLDriverWrapper,setupRepl:ReplId-26113-21226-c29bd-0: finished to load,368e25dc,<*> finished to load,[]
201,20/02/28,22:47:51,INFO,SQLDriverWrapper,setupRepl:ReplId-6fffc-24c49-c4ef9-6: finished to load,368e25dc,<*> finished to load,[]
202,20/02/28,22:47:51,INFO,JettyClient$,"Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DriverDaemon), idleTimeout=2 hours, useBlockingConnect: true",00f8393c,"Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, <*> idleTimeout=2 hours, useBlockingConnect: true",[]
203,20/02/28,22:47:51,INFO,DriverCorral,Starting sql repl ReplId-7f7de-3f667-b27c3-1,6b45de39,Starting sql repl <*>,[]
204,20/02/28,22:47:51,WARN,SQLDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
205,20/02/28,22:47:51,INFO,SQLDriverWrapper,setupRepl:ReplId-7f7de-3f667-b27c3-1: finished to load,368e25dc,<*> finished to load,[]
206,20/02/28,22:47:51,INFO,DriverCorral,Starting sql repl ReplId-7b683-976ad-dfc7c-f,6b45de39,Starting sql repl <*>,[]
207,20/02/28,22:47:51,WARN,SQLDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
208,20/02/28,22:47:51,INFO,SQLDriverWrapper,setupRepl:ReplId-7b683-976ad-dfc7c-f: finished to load,368e25dc,<*> finished to load,[]
209,20/02/28,22:47:51,INFO,DriverCorral,Starting sql repl ReplId-19711-fca89-c26d6,6b45de39,Starting sql repl <*>,[]
210,20/02/28,22:47:51,WARN,SQLDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
211,20/02/28,22:47:51,INFO,SQLDriverWrapper,setupRepl:ReplId-19711-fca89-c26d6: finished to load,368e25dc,<*> finished to load,[]
212,20/02/28,22:47:51,INFO,DriverCorral,Starting r repl ReplId-3593d-ebddd-8b23a-e,c2102708,Starting r repl ReplId-3593d-ebddd-8b23a-e,[]
213,20/02/28,22:47:51,WARN,RDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
214,20/02/28,22:47:51,INFO,RDriverLocal,1. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: object created with for ReplId-3593d-ebddd-8b23a-e.,4092ff35,1. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: object created with for ReplId-3593d-ebddd-8b23a-e.,[]
215,20/02/28,22:47:51,INFO,RDriverLocal,2. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: initializing ...,a50bc5a8,2. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: initializing ...,[]
216,20/02/28,22:47:52,INFO,RDriverLocal,3. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: started RBackend thread on port 41274,e43a7a5b,3. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: started RBackend thread on port 41274,[]
217,20/02/28,22:47:52,INFO,RDriverLocal,4. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: waiting for SparkR to be installed ...,eb63dba5,4. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: waiting for SparkR to be installed ...,[]
218,20/02/28,22:47:52,INFO,DriverCorral,Starting python repl ReplId-8497b-f4c2d-af4d9,642ab912,Starting python repl ReplId-8497b-f4c2d-af4d9,[]
219,20/02/28,22:47:52,WARN,PythonDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
220,20/02/28,22:47:52,INFO,PythonDriverLocal,Starting gateway server for repl ReplId-8497b-f4c2d-af4d9,109b3335,Starting gateway server for repl ReplId-8497b-f4c2d-af4d9,[]
221,20/02/28,22:47:52,INFO,Utils,"resolved command to be run: List(virtualenv, /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b, --no-site-packages, -p, /databricks/python/bin/python, --no-setuptools, --no-wheel, --no-pip)",71bc67d1,"resolved command to be run: List(virtualenv, /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b, --no-site-packages, -p, /databricks/python/bin/python, --no-setuptools, --no-wheel, --no-pip)",[]
222,20/02/28,22:47:52,INFO,DriverCorral,Starting scala repl ReplId-641e1-43796-385e1-8,e147e61a,Starting scala repl ReplId-641e1-43796-385e1-8,[]
223,20/02/28,22:47:52,WARN,ScalaDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
224,20/02/28,22:47:52,INFO,ScalaDriverWrapper,setupRepl:ReplId-641e1-43796-385e1-8: finished to load,368e25dc,<*> finished to load,[]
225,20/02/28,22:47:52,INFO,DriverCorral,Starting sql repl ReplId-4703f-a6518-7dd28-9,6b45de39,Starting sql repl <*>,[]
226,20/02/28,22:47:52,WARN,SQLDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
227,20/02/28,22:47:52,INFO,SQLDriverWrapper,setupRepl:ReplId-4703f-a6518-7dd28-9: finished to load,368e25dc,<*> finished to load,[]
228,20/02/28,22:47:53,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_8003361428351860078_972c5bd7-c187-4c99-ab26-32425d02c2dc,60154ad1,Added result fetcher for <*>,[]
229,20/02/28,22:47:53,INFO,ProgressReporter$,Added result fetcher for 7214225882070146584_6622863124546160429_3bab45f008c44d3b8a54b200eb043826,60154ad1,Added result fetcher for <*>,[]
230,20/02/28,22:47:53,INFO,DatabricksUtils,created python virtualenv: /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b,8e3b1fd8,created python virtualenv: /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b,[]
231,20/02/28,22:47:53,INFO,Utils,"resolved command to be run: List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/bin/python, -c, from distutils.sysconfig import get_python_lib; print(get_python_lib()))",e79f3eaa,"resolved command to be run: List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/bin/python, -c, from distutils.sysconfig import get_python_lib; print(get_python_lib()))",[]
232,20/02/28,22:47:53,INFO,SignalUtils,Registered signal handler for INT,d066e11a,Registered signal handler for INT,[]
233,20/02/28,22:47:53,INFO,Utils,"resolved command to be run: List(/databricks/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))",2a3ccd9f,"resolved command to be run: List(/databricks/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))",[]
234,20/02/28,22:47:53,INFO,DatabricksUtils,created sitecustomize.py at /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/lib/python3.7/sitecustomize.py,957c2090,created sitecustomize.py at /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/lib/python3.7/sitecustomize.py,[]
235,20/02/28,22:47:53,INFO,PythonDriverLocal$,Time spent to start virtualenv /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b is 1090,89303877,Time spent to start virtualenv /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b is 1090,[]
236,20/02/28,22:47:53,INFO,PythonDriverLocal$,"Python process builder: [/databricks/spark/python/pyspark/wrapped_python.py, root, /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/bin/python, -u, /local_disk0/tmp/1582930072342-0/PythonShell.py, 43339, 0, 50000, 2177, 218164f3f65d4a4ca63276a38e458e45, 2.4.4, 0bece102c3a57c7e446912d4a03cdd4feaf1aacc0a97cdf63fb5412c44f02820]",1bdee7df,"Python process builder: [/databricks/spark/python/pyspark/wrapped_python.py, root, /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/bin/python, -u, /local_disk0/tmp/1582930072342-0/PythonShell.py, 43339, 0, 50000, 2177, 218164f3f65d4a4ca63276a38e458e45, 2.4.4, 0bece102c3a57c7e446912d4a03cdd4feaf1aacc0a97cdf63fb5412c44f02820]",[]
237,20/02/28,22:47:53,INFO,PythonDriverLocal$,"Cgroup isolation disabled, not placing python process in repl cgroup",9f15ff7c,"Cgroup isolation disabled, not placing python process in repl cgroup",[]
238,20/02/28,22:47:54,INFO,LogicalPlanStats,Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$,9a561085,Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$,[]
239,20/02/28,22:47:55,INFO,DriverILoop,Set class prefix to: line49b319df9593406199cec71f0268458a,f59cda88,Set class prefix to: line49b319df9593406199cec71f0268458a,[]
240,20/02/28,22:47:55,INFO,DriverILoop,set ContextClassLoader,16caeedd,set ContextClassLoader,[]
241,20/02/28,22:47:55,INFO,DriverILoop,initialized intp,679bf9b9,initialized intp,[]
242,20/02/28,22:47:56,INFO,HiveUtils,Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/daemon--data--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:file:/databricks/hive/third_party--jackson--guava_only_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/databricks/hive/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:file:/databricks/hive/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:file:/databricks/hive/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/databricks/hive/s3commit--common--common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/databricks/hive/third_party--jackson--paranamer_only_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/common--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-storage__7.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/databricks/hive/third_party--jetty-client--jetty-util_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:file:/databricks/hive/----jackson_databind_shaded--libjackson-databind.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.42.Final.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:file:/databricks/hive/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:file:/databricks/hive/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:file:/databricks/hive/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:file:/databricks/hive/third_party--jetty-client--jetty-io_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/s3commit--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/hive/common--credentials--credentials-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:file:/databricks/hive/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-jetty9-hadoop1_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:file:/databricks/hive/extern--extern-spark_2.4_2.11_deploy.jar:file:/databricks/hive/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:file:/databricks/hive/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/hive/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar:file:/databricks/hive/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:file:/databricks/hive/third_party--jetty-client--jetty-client_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/databricks/hive/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/databricks/hive/third_party--jetty-client--jetty-http_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:file:/databricks/hive/s3--s3-spark_2.4_2.11_deploy.jar:file:/databricks/hive/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:file:/databricks/hive/----jackson_annotations_shaded--libjackson-annotations.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:file:/databricks/hive/common--path--path-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/databricks/hive/common--lazy--lazy-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:file:/databricks/hive/api-base--api-base-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/databricks/hive/api-base--api-base_java-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/databricks/hive/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--jackson--jsr305_only_shaded.jar:file:/databricks/hive/extern--libaws-regions.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/databricks/hive/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/databricks/hive/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/databricks/hive/common--jetty--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/----jackson_core_shaded--libjackson-core.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/bonecp-configs.jar,9e9af40d,Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/daemon--data--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:file:/databricks/hive/third_party--jackson--guava_only_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/databricks/hive/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:file:/databricks/hive/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:file:/databricks/hive/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/databricks/hive/s3commit--common--common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/databricks/hive/third_party--jackson--paranamer_only_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/common--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-storage__7.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/databricks/hive/third_party--jetty-client--jetty-util_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:file:/databricks/hive/----jackson_databind_shaded--libjackson-databind.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.42.Final.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:file:/databricks/hive/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:file:/databricks/hive/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:file:/databricks/hive/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:file:/databricks/hive/third_party--jetty-client--jetty-io_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/s3commit--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/hive/common--credentials--credentials-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:file:/databricks/hive/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-jetty9-hadoop1_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:file:/databricks/hive/extern--extern-spark_2.4_2.11_deploy.jar:file:/databricks/hive/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:file:/databricks/hive/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/hive/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar:file:/databricks/hive/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:file:/databricks/hive/third_party--jetty-client--jetty-client_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/databricks/hive/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/databricks/hive/third_party--jetty-client--jetty-http_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:file:/databricks/hive/s3--s3-spark_2.4_2.11_deploy.jar:file:/databricks/hive/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:file:/databricks/hive/----jackson_annotations_shaded--libjackson-annotations.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:file:/databricks/hive/common--path--path-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/databricks/hive/common--lazy--lazy-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:file:/databricks/hive/api-base--api-base-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/databricks/hive/api-base--api-base_java-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/databricks/hive/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--jackson--jsr305_only_shaded.jar:file:/databricks/hive/extern--libaws-regions.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/databricks/hive/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/databricks/hive/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/databricks/hive/common--jetty--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/----jackson_core_shaded--libjackson-core.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/bonecp-configs.jar,[]
243,20/02/28,22:47:57,INFO,AsyncEventQueue,"Process of event SparkListenerSQLExecutionStart(0,show databases,org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)",f95d0c04,"Process of event SparkListenerSQLExecutionStart(0,show databases,org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)",[]
244,20/02/28,22:47:57,INFO,HiveMetaStore,0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore,c513a2c4,<*> Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore,[]
245,20/02/28,22:47:57,INFO,ObjectStore,"ObjectStore, initialize called",f97d2395,"ObjectStore, initialize called",[]
246,20/02/28,22:47:58,INFO,Persistence,Property hive.metastore.integral.jdo.pushdown unknown - will be ignored,187a75f5,Property <*> unknown - will be ignored,[]
247,20/02/28,22:47:58,INFO,Persistence,Property datanucleus.cache.level2 unknown - will be ignored,187a75f5,Property <*> unknown - will be ignored,[]
248,20/02/28,22:47:59,INFO,ObjectStore,"Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""",b90681cc,"Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""",[]
249,20/02/28,22:47:59,INFO,PythonDriverLocal,PythonDriver[ReplId-8497b-f4c2d-af4d9](stateStr) Set Python Interp in JVM for ReplId-8497b-f4c2d-af4d9,95af587e,PythonDriver[ReplId-8497b-f4c2d-af4d9](stateStr) Set Python Interp in JVM for ReplId-8497b-f4c2d-af4d9,[]
250,20/02/28,22:47:59,INFO,PythonDriverWrapper,setupRepl:ReplId-8497b-f4c2d-af4d9: finished to load,368e25dc,<*> finished to load,[]
251,20/02/28,22:48:01,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
252,20/02/28,22:48:01,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
253,20/02/28,22:48:01,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
254,20/02/28,22:48:01,INFO,Datastore,"The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.",5592bcff,"The class <*> is tagged as ""embedded-only"" so does not have its own datastore table.",[]
255,20/02/28,22:48:01,INFO,Query,"Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing",af819f61,"Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing",[]
256,20/02/28,22:48:01,INFO,ObjectStore,Initialized ObjectStore,fee37921,Initialized ObjectStore,[]
257,20/02/28,22:48:01,INFO,HiveMetaStore,Added admin role in metastore,36a330ca,Added <*> role in metastore,[]
258,20/02/28,22:48:01,INFO,HiveMetaStore,Added public role in metastore,36a330ca,Added <*> role in metastore,[]
259,20/02/28,22:48:01,INFO,HiveMetaStore,"No user is added in admin role, since config is empty",ba641907,"No user is added in admin role, since config is empty",[]
260,20/02/28,22:48:02,INFO,SessionState,No Tez session required at this point. hive.execution.engine=mr.,4bb9da49,No Tez session required at this point. hive.execution.engine=mr.,[]
261,20/02/28,22:48:02,INFO,HiveClientImpl,Warehouse location for Hive client (version 0.13.1) is /user/hive/warehouse,0337ff6b,Warehouse location for Hive client (version <*> is /user/hive/warehouse,[]
262,20/02/28,22:48:02,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
263,20/02/28,22:48:02,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
264,20/02/28,22:48:02,INFO,HiveMetaStore,0: get_databases: *,cc60c8fa,0: get_databases: *,[]
265,20/02/28,22:48:02,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_databases: *,c205b170,ugi=root ip=unknown-ip-addr cmd=get_databases: *,[]
266,20/02/28,22:48:04,INFO,AsyncEventQueue,"Process of event SparkListenerSQLUsageLogging(0,1582930082300,CallSite(sql at SQLDriverLocal.scala:88,org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)",dd0b989b,"Process of event SparkListenerSQLUsageLogging(0,1582930082300,CallSite(sql at SQLDriverLocal.scala:88,org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)",[]
267,20/02/28,22:48:04,INFO,SQLAppStatusListener,Execution ID: 0 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
268,20/02/28,22:48:04,INFO,SQLAppStatusListener,Execution ID: 1 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
269,20/02/28,22:48:04,INFO,CodeGenerator,Code generated in 803.712518 ms,67696ad5,Code generated in <*> ms,[]
270,20/02/28,22:48:04,INFO,CodeGenerator,Code generated in 64.586368 ms,67696ad5,Code generated in <*> ms,[]
271,20/02/28,22:48:04,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_8003361428351860078_972c5bd7-c187-4c99-ab26-32425d02c2dc,0d9ed607,Removed result fetcher for <*>,[]
272,20/02/28,22:48:05,INFO,SQLAppStatusListener,Execution ID: 2 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
273,20/02/28,22:48:05,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_6510401489672478420_9e0d4829-5908-405b-915b-df5149c76d2c,60154ad1,Added result fetcher for <*>,[]
274,20/02/28,22:48:06,INFO,HiveMetaStore,0: get_database: global_temp,2110a61b,0: get_database: global_temp,[]
275,20/02/28,22:48:06,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp,12014f88,ugi=root ip=unknown-ip-addr cmd=get_database: global_temp,[]
276,20/02/28,22:48:06,ERROR,RetryingHMSHandler,NoSuchObjectException(message:There is no database named global_temp),9abbc502,NoSuchObjectException(message:There is no database named global_temp),[]
277,20/02/28,22:48:06,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
278,20/02/28,22:48:06,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
279,20/02/28,22:48:06,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
280,20/02/28,22:48:06,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
281,20/02/28,22:48:06,INFO,HiveMetaStore,0: get_tables: db=default pat=*,c2534b22,0: <*> db=default pat=*,[]
282,20/02/28,22:48:06,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*,a5ec3f8e,ugi=root ip=unknown-ip-addr <*> db=default pat=*,[]
283,20/02/28,22:48:06,INFO,SQLAppStatusListener,Execution ID: 3 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
284,20/02/28,22:48:06,INFO,SQLAppStatusListener,Execution ID: 4 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
285,20/02/28,22:48:06,INFO,CodeGenerator,Code generated in 23.73242 ms,67696ad5,Code generated in <*> ms,[]
286,20/02/28,22:48:06,INFO,SQLAppStatusListener,Execution ID: 5 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
287,20/02/28,22:48:06,INFO,CodeGenerator,Code generated in 21.351697 ms,67696ad5,Code generated in <*> ms,[]
288,20/02/28,22:48:06,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_6510401489672478420_9e0d4829-5908-405b-915b-df5149c76d2c,0d9ed607,Removed result fetcher for <*>,[]
289,20/02/28,22:48:06,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_8863609662602837032_4cdd00ca-2f61-4026-8edc-588d2c885897,60154ad1,Added result fetcher for <*>,[]
290,20/02/28,22:48:07,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
291,20/02/28,22:48:07,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
292,20/02/28,22:48:07,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_final,9ab6297d,0: get_table : db=default <*>,[]
293,20/02/28,22:48:07,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
294,20/02/28,22:48:07,INFO,ProgressReporter$,Removed result fetcher for 7214225882070146584_6622863124546160429_3bab45f008c44d3b8a54b200eb043826,0d9ed607,Removed result fetcher for <*>,[]
295,20/02/28,22:48:07,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_final,9ab6297d,0: get_table : db=default <*>,[]
296,20/02/28,22:48:07,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
297,20/02/28,22:48:07,INFO,SQLAppStatusListener,Execution ID: 6 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
298,20/02/28,22:48:07,INFO,SQLAppStatusListener,Execution ID: 7 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
299,20/02/28,22:48:08,INFO,CodeGenerator,Code generated in 34.800637 ms,67696ad5,Code generated in <*> ms,[]
300,20/02/28,22:48:08,INFO,SQLAppStatusListener,Execution ID: 8 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
301,20/02/28,22:48:08,INFO,CodeGenerator,Code generated in 23.702847 ms,67696ad5,Code generated in <*> ms,[]
302,20/02/28,22:48:08,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_8863609662602837032_4cdd00ca-2f61-4026-8edc-588d2c885897,0d9ed607,Removed result fetcher for <*>,[]
303,20/02/28,22:48:08,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_7301126850103711220_1c069279-6fad-481c-97db-f2a621e86f7f,60154ad1,Added result fetcher for <*>,[]
304,20/02/28,22:48:08,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
305,20/02/28,22:48:08,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
306,20/02/28,22:48:08,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_20200215,9ab6297d,0: get_table : db=default <*>,[]
307,20/02/28,22:48:08,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
308,20/02/28,22:48:08,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_20200215,9ab6297d,0: get_table : db=default <*>,[]
309,20/02/28,22:48:08,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
310,20/02/28,22:48:09,INFO,SQLAppStatusListener,Execution ID: 9 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
311,20/02/28,22:48:09,INFO,SQLAppStatusListener,Execution ID: 10 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
312,20/02/28,22:48:09,INFO,SQLAppStatusListener,Execution ID: 11 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
313,20/02/28,22:48:09,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_7301126850103711220_1c069279-6fad-481c-97db-f2a621e86f7f,0d9ed607,Removed result fetcher for <*>,[]
314,20/02/28,22:48:17,INFO,RDriverLocal$,SparkR installation completed.,8ad03cda,SparkR installation completed.,[]
315,20/02/28,22:48:17,INFO,RDriverLocal,5. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: launching R process ...,01555867,5. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: launching R process ...,[]
316,20/02/28,22:48:17,INFO,RDriverLocal,"6. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: cgroup isolation disabled, not placing R process in REPL cgroup.",ec482961,"6. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: cgroup isolation disabled, not placing R process in REPL cgroup.",[]
317,20/02/28,22:48:17,INFO,RDriverLocal,7. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: starting R process on port 1100 (attempt 1) ...,00a15d24,7. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: starting R process on port 1100 (attempt 1) ...,[]
318,20/02/28,22:48:17,INFO,RDriverLocal,8. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: setting up BufferedStreamThread with bufferSize: 100.,09862f36,8. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: setting up BufferedStreamThread with bufferSize: 100.,[]
319,20/02/28,22:48:18,INFO,RDriverLocal,9. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: R process started with RServe listening on port 1100.,37511901,9. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: R process started with RServe listening on port 1100.,[]
320,20/02/28,22:48:19,INFO,RDriverLocal,10. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: starting interpreter to talk to R process ...,6a8ae455,10. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: starting interpreter to talk to R process ...,[]
321,20/02/28,22:48:20,WARN,SparkContext,Using an existing SparkContext; some configuration may not take effect.,3794d0ab,Using an existing SparkContext; some configuration may not take effect.,[]
322,20/02/28,22:48:20,INFO,RDriverLocal,11. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: R interpretter is connected.,3056bd2b,11. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: R interpretter is connected.,[]
323,20/02/28,22:48:20,INFO,RDriverWrapper,setupRepl:ReplId-3593d-ebddd-8b23a-e: finished to load,368e25dc,<*> finished to load,[]
324,20/02/28,22:50:09,INFO,ContextCleaner,Cleaned accumulator 2 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
325,20/02/28,22:50:09,INFO,ContextCleaner,Cleaned accumulator 3 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
326,20/02/28,22:50:09,INFO,ContextCleaner,Cleaned accumulator 4 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
327,20/02/28,22:50:09,INFO,ContextCleaner,Cleaned accumulator 1 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
328,20/02/28,22:52:31,INFO,DriverCorral,DBFS health check ok,789df238,DBFS health check ok,[]
329,20/02/28,22:52:32,INFO,HikariDataSource,metastore-monitor - Starting...,927bc97a,metastore-monitor - Starting...,[]
330,20/02/28,22:52:32,INFO,HikariDataSource,metastore-monitor - Start completed.,5b7e5d94,metastore-monitor - <*> completed.,[]
331,20/02/28,22:52:32,INFO,HikariDataSource,metastore-monitor - Shutdown initiated...,79466fcb,metastore-monitor - Shutdown initiated...,[]
332,20/02/28,22:52:32,INFO,HikariDataSource,metastore-monitor - Shutdown completed.,5b7e5d94,metastore-monitor - <*> completed.,[]
333,20/02/28,22:52:32,INFO,MetastoreMonitor,Metastore healthcheck successful (connection duration = 20 milliseconds),a68c364d,Metastore healthcheck successful (connection duration = <*> milliseconds),[]
334,20/02/28,22:52:49,INFO,HiveMetaStore,1: get_database: default,cf605cef,1: get_database: default,[]
335,20/02/28,22:52:49,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
336,20/02/28,22:52:49,INFO,HiveMetaStore,1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore,c513a2c4,<*> Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore,[]
337,20/02/28,22:52:49,INFO,ObjectStore,"ObjectStore, initialize called",f97d2395,"ObjectStore, initialize called",[]
338,20/02/28,22:52:49,INFO,Query,"Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing",af819f61,"Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing",[]
339,20/02/28,22:52:49,INFO,ObjectStore,Initialized ObjectStore,fee37921,Initialized ObjectStore,[]
340,20/02/28,22:52:49,INFO,DriverCorral,Metastore health check ok,5d8b1e0a,Metastore health check ok,[]
341,20/02/28,22:53:11,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_8575408154924242561_1aabdc66-d2b7-488e-8627-95614094595d,60154ad1,Added result fetcher for <*>,[]
342,20/02/28,22:53:11,INFO,HiveMetaStore,0: get_databases: *,cc60c8fa,0: get_databases: *,[]
343,20/02/28,22:53:11,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_databases: *,c205b170,ugi=root ip=unknown-ip-addr cmd=get_databases: *,[]
344,20/02/28,22:53:11,INFO,SQLAppStatusListener,Execution ID: 12 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
345,20/02/28,22:53:11,INFO,SQLAppStatusListener,Execution ID: 13 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
346,20/02/28,22:53:11,INFO,CodeGenerator,Code generated in 17.110219 ms,67696ad5,Code generated in <*> ms,[]
347,20/02/28,22:53:11,INFO,SQLAppStatusListener,Execution ID: 14 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
348,20/02/28,22:53:11,INFO,CodeGenerator,Code generated in 15.345658 ms,67696ad5,Code generated in <*> ms,[]
349,20/02/28,22:53:11,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_8575408154924242561_1aabdc66-d2b7-488e-8627-95614094595d,0d9ed607,Removed result fetcher for <*>,[]
350,20/02/28,22:53:12,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_5622342512599412368_13f4066b-a605-4bf5-812a-05779a9cd787,60154ad1,Added result fetcher for <*>,[]
351,20/02/28,22:53:12,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
352,20/02/28,22:53:12,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
353,20/02/28,22:53:12,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
354,20/02/28,22:53:12,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
355,20/02/28,22:53:12,INFO,HiveMetaStore,0: get_tables: db=default pat=*,c2534b22,0: <*> db=default pat=*,[]
356,20/02/28,22:53:12,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*,a5ec3f8e,ugi=root ip=unknown-ip-addr <*> db=default pat=*,[]
357,20/02/28,22:53:12,INFO,SQLAppStatusListener,Execution ID: 15 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
358,20/02/28,22:53:12,INFO,SQLAppStatusListener,Execution ID: 16 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
359,20/02/28,22:53:12,INFO,CodeGenerator,Code generated in 22.147679 ms,67696ad5,Code generated in <*> ms,[]
360,20/02/28,22:53:12,INFO,SQLAppStatusListener,Execution ID: 17 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
361,20/02/28,22:53:12,INFO,CodeGenerator,Code generated in 28.274594 ms,67696ad5,Code generated in <*> ms,[]
362,20/02/28,22:53:12,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_5622342512599412368_13f4066b-a605-4bf5-812a-05779a9cd787,0d9ed607,Removed result fetcher for <*>,[]
363,20/02/28,22:53:13,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_8172190809674484735_a05caece-40ae-41b5-8b9a-2680835f69cb,60154ad1,Added result fetcher for <*>,[]
364,20/02/28,22:53:13,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
365,20/02/28,22:53:13,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
366,20/02/28,22:53:13,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_final,9ab6297d,0: get_table : db=default <*>,[]
367,20/02/28,22:53:13,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
368,20/02/28,22:53:13,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_final,9ab6297d,0: get_table : db=default <*>,[]
369,20/02/28,22:53:13,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
370,20/02/28,22:53:13,INFO,SQLAppStatusListener,Execution ID: 18 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
371,20/02/28,22:53:13,INFO,SQLAppStatusListener,Execution ID: 19 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
372,20/02/28,22:53:14,INFO,CodeGenerator,Code generated in 24.176687 ms,67696ad5,Code generated in <*> ms,[]
373,20/02/28,22:53:14,INFO,SQLAppStatusListener,Execution ID: 20 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
374,20/02/28,22:53:14,INFO,CodeGenerator,Code generated in 30.949684 ms,67696ad5,Code generated in <*> ms,[]
375,20/02/28,22:53:14,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_8172190809674484735_a05caece-40ae-41b5-8b9a-2680835f69cb,0d9ed607,Removed result fetcher for <*>,[]
376,20/02/28,22:53:17,INFO,ProgressReporter$,Added result fetcher for 5117208913732555401_7722394248052958781_305af185-7f94-4ba9-a13b-1be60e8ede20,60154ad1,Added result fetcher for <*>,[]
377,20/02/28,22:53:17,INFO,HiveMetaStore,0: get_database: default,ef63b8b7,0: <*> default,[]
378,20/02/28,22:53:17,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
379,20/02/28,22:53:17,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_20200215,9ab6297d,0: get_table : db=default <*>,[]
380,20/02/28,22:53:17,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
381,20/02/28,22:53:17,INFO,HiveMetaStore,0: get_table : db=default tbl=visits_20200215,9ab6297d,0: get_table : db=default <*>,[]
382,20/02/28,22:53:17,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215,3cb90984,ugi=root ip=unknown-ip-addr cmd=get_table : db=default <*>,[]
383,20/02/28,22:53:17,INFO,SQLAppStatusListener,Execution ID: 21 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
384,20/02/28,22:53:17,INFO,SQLAppStatusListener,Execution ID: 22 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
385,20/02/28,22:53:17,INFO,SQLAppStatusListener,Execution ID: 23 Total Executor Run Time: 0,52999028,Execution ID: <*> Total Executor Run Time: 0,[]
386,20/02/28,22:53:17,INFO,ProgressReporter$,Removed result fetcher for 5117208913732555401_7722394248052958781_305af185-7f94-4ba9-a13b-1be60e8ede20,0d9ed607,Removed result fetcher for <*>,[]
387,20/02/28,22:53:18,INFO,DriverCorral,Starting sql repl ReplId-6b5a9-7ed4d-b78b2-b,6b45de39,Starting sql repl <*>,[]
388,20/02/28,22:53:18,WARN,SQLDriverLocal,loadLibraries: Libraries failed to be installed: Set(),975cab90,loadLibraries: Libraries failed to be installed: Set(),[]
389,20/02/28,22:53:18,INFO,SQLDriverWrapper,setupRepl:ReplId-6b5a9-7ed4d-b78b2-b: finished to load,368e25dc,<*> finished to load,[]
390,20/02/28,22:53:18,INFO,ProgressReporter$,Added result fetcher for 597144716774667481_6780863106840874462_1b0a48ee728b41b49255c04182eca8a0,60154ad1,Added result fetcher for <*>,[]
391,20/02/28,22:57:32,INFO,DriverCorral,DBFS health check ok,789df238,DBFS health check ok,[]
392,20/02/28,22:57:32,INFO,HikariDataSource,metastore-monitor - Starting...,927bc97a,metastore-monitor - Starting...,[]
393,20/02/28,22:57:32,INFO,HikariDataSource,metastore-monitor - Start completed.,5b7e5d94,metastore-monitor - <*> completed.,[]
394,20/02/28,22:57:32,INFO,HikariDataSource,metastore-monitor - Shutdown initiated...,79466fcb,metastore-monitor - Shutdown initiated...,[]
395,20/02/28,22:57:32,INFO,HikariDataSource,metastore-monitor - Shutdown completed.,5b7e5d94,metastore-monitor - <*> completed.,[]
396,20/02/28,22:57:32,INFO,MetastoreMonitor,Metastore healthcheck successful (connection duration = 30 milliseconds),a68c364d,Metastore healthcheck successful (connection duration = <*> milliseconds),[]
397,20/02/28,22:57:49,INFO,HiveMetaStore,1: get_database: default,cf605cef,1: get_database: default,[]
398,20/02/28,22:57:49,INFO,audit,ugi=root	ip=unknown-ip-addr	cmd=get_database: default,1b7c4b8a,ugi=root ip=unknown-ip-addr <*> default,[]
399,20/02/28,22:57:49,INFO,DriverCorral,Metastore health check ok,5d8b1e0a,Metastore health check ok,[]
400,20/02/28,22:58:46,INFO,ProgressReporter$,Removed result fetcher for 597144716774667481_6780863106840874462_1b0a48ee728b41b49255c04182eca8a0,0d9ed607,Removed result fetcher for <*>,[]
401,20/02/28,22:58:46,INFO,ProgressReporter$,Added result fetcher for 7214225882070146584_8096105091980147949_1b0a48ee728b41b49255c04182eca8a0,60154ad1,Added result fetcher for <*>,[]
402,20/02/28,22:58:46,INFO,ProgressReporter$,Removed result fetcher for 7214225882070146584_8096105091980147949_1b0a48ee728b41b49255c04182eca8a0,0d9ed607,Removed result fetcher for <*>,[]
403,20/02/28,22:58:46,INFO,ProgressReporter$,Added result fetcher for 7214225882070146584_8489408173556252300_1b0a48ee728b41b49255c04182eca8a0,60154ad1,Added result fetcher for <*>,[]
404,20/02/28,22:59:19,INFO,DbfsOutputStream,"DbfsOutputStream closed, reporting metrics.",6647097a,"DbfsOutputStream closed, reporting metrics.",[]
405,20/02/28,22:59:19,INFO,ProgressReporter$,Removed result fetcher for 7214225882070146584_8489408173556252300_1b0a48ee728b41b49255c04182eca8a0,0d9ed607,Removed result fetcher for <*>,[]
406,20/02/28,22:59:19,INFO,ProgressReporter$,Added result fetcher for 7214225882070146584_6575950616243125849_1b0a48ee728b41b49255c04182eca8a0,60154ad1,Added result fetcher for <*>,[]
407,20/02/28,22:59:32,INFO,ContextCleaner,Cleaned accumulator 5 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
408,20/02/28,22:59:32,INFO,ContextCleaner,Cleaned accumulator 8 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
409,20/02/28,22:59:32,INFO,ContextCleaner,Cleaned accumulator 6 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
410,20/02/28,22:59:32,INFO,ContextCleaner,Cleaned accumulator 7 (name: number of output rows),a8026567,Cleaned accumulator <*> (name: number of output rows),[]
