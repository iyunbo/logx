20/02/28 22:47:30 INFO StaticConf$: DB_HOME: /databricks
20/02/28 22:47:30 INFO DriverDaemon$: ========== driver starting up ==========
20/02/28 22:47:30 INFO DriverDaemon$: Java: Private Build 1.8.0_232
20/02/28 22:47:30 INFO DriverDaemon$: OS: Linux/amd64 4.4.0-1102-aws
20/02/28 22:47:30 INFO DriverDaemon$: CWD: /databricks/driver
20/02/28 22:47:30 INFO DriverDaemon$: Mem: Max: 4.8G loaded GCs: PS Scavenge, PS MarkSweep
20/02/28 22:47:30 INFO DriverDaemon$: Logging multibyte characters: âœ“
20/02/28 22:47:30 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
20/02/28 22:47:30 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
20/02/28 22:47:30 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
20/02/28 22:47:30 INFO DriverDaemon$: == Modules:
20/02/28 22:47:31 INFO DriverDaemon$: Starting prometheus metrics log export timer
20/02/28 22:47:31 INFO DriverDaemon$: Universe Git Hash: 687e04cafbfe88e512c6aea9e4f53a2c4e1d5689
20/02/28 22:47:31 INFO DriverDaemon$: Spark Git Hash: 428199fa1bdaea59715ade6e7d4d6bb5a11c388d
20/02/28 22:47:31 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)
20/02/28 22:47:31 INFO DatabricksILoop$: Creating throwaway interpreter
20/02/28 22:47:31 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
20/02/28 22:47:31 INFO MetastoreMonitor$: Internal internal metastore configured (config=DbMetastoreConfig{host=devtierprod1-db.caj77bnxuhme.us-west-2.rds.amazonaws.com, port=3306, dbName=organization823618267676840, user=YbAhhCtN6Tgu0BzS})
20/02/28 22:47:32 INFO HikariDataSource: metastore-monitor - Starting...
20/02/28 22:47:32 INFO HikariDataSource: metastore-monitor - Start completed.
20/02/28 22:47:32 INFO DriverCorral: Creating the driver context
20/02/28 22:47:32 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-3281211262966016233-8fe57c53-b822-4434-871c-c263ce4408f3
20/02/28 22:47:32 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
20/02/28 22:47:32 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
20/02/28 22:47:32 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
20/02/28 22:47:32 INFO HikariDataSource: metastore-monitor - Shutdown completed.
20/02/28 22:47:32 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 941 milliseconds)
20/02/28 22:47:32 INFO SparkContext: Running Spark version 2.4.4
20/02/28 22:47:33 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
20/02/28 22:47:33 INFO SparkContext: Submitted application: Databricks Shell
20/02/28 22:47:33 INFO SecurityManager: Changing view acls to: root
20/02/28 22:47:33 INFO SecurityManager: Changing modify acls to: root
20/02/28 22:47:33 INFO SecurityManager: Changing view acls groups to: 
20/02/28 22:47:33 INFO SecurityManager: Changing modify acls groups to: 
20/02/28 22:47:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/02/28 22:47:34 INFO Utils: Successfully started service 'sparkDriver' on port 34230.
20/02/28 22:47:34 INFO SparkEnv: Registering MapOutputTracker
20/02/28 22:47:34 INFO SparkEnv: Registering BlockManagerMaster
20/02/28 22:47:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/02/28 22:47:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/02/28 22:47:34 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-8d21e9b3-3221-4782-821f-fca3197f1480
20/02/28 22:47:34 INFO MemoryStore: MemoryStore started with capacity 2.5 GB
20/02/28 22:47:34 INFO SparkEnv: Registering OutputCommitCoordinator
20/02/28 22:47:34 INFO SparkContext: Spark configuration:
eventLog.rolloverIntervalSeconds=3600
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.cloudProvider=AWS
spark.databricks.clusterSource=UI
spark.databricks.clusterUsageTags.autoTerminationMinutes=120
spark.databricks.clusterUsageTags.cloudProvider=AWS
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Name","value":"ce4-worker"}]
spark.databricks.clusterUsageTags.clusterAvailability=ON_DEMAND
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterEbsVolumeCount=0
spark.databricks.clusterUsageTags.clusterEbsVolumeSize=0
spark.databricks.clusterUsageTags.clusterEbsVolumeType=GENERAL_PURPOSE_SSD
spark.databricks.clusterUsageTags.clusterFirstOnDemand=0
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=0228-224707-rangy181
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=false
spark.databricks.clusterUsageTags.clusterLogDestination=
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterName=My Cluster
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=dev-tier-node
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=823618267676840
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=fixed_size
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidPricePercent=100
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=0
spark.databricks.clusterUsageTags.clusterWorkers=0
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.containerZoneId=us-west-2c
spark.databricks.clusterUsageTags.dataPlaneRegion=us-west-2
spark.databricks.clusterUsageTags.driverContainerId=5dd6a834db124a0a8c7c61157ce42996
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.172.228.50
spark.databricks.clusterUsageTags.driverInstanceId=i-0618058d5d73e4e0a
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.172.231.9
spark.databricks.clusterUsageTags.driverNodeType=dev-tier-node
spark.databricks.clusterUsageTags.driverPublicDns=ec2-34-220-208-104.us-west-2.compute.amazonaws.com
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=false
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.instanceBootstrapType=ssh
spark.databricks.clusterUsageTags.instanceWorkerEnvId=default-worker-env
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.region=us-west-2
spark.databricks.clusterUsageTags.sparkVersion=6.2.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=default-worker-env
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNodeTypeId=dev-tier-node
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.glue.credentialsProviderFactoryClassName=*********(redacted)
spark.databricks.passthrough.glue.executorServiceFactoryClassName=com.databricks.backend.daemon.driver.GlueClientExecutorServiceFactory
spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class=com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=3281211262966016233
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.S3LockBasedLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=dev-tier-node
spark.driver.allowMultipleContexts=false
spark.driver.host=10.172.228.50
spark.driver.maxResultSize=4g
spark.driver.port=34230
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=4800m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v1
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.block.size.row.check.max=10
spark.hadoop.parquet.block.size.row.check.min=10
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.parquet.page.size.check.estimate=false
spark.hadoop.parquet.page.verify-checksum.enabled=true
spark.hadoop.parquet.page.write-checksum.enabled=true
spark.hadoop.spark.driverproxy.customHeadersToProperties=*********(redacted)
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.closeSessionHeaderName=X-Databricks-SqlService-CloseSession
spark.home=/databricks/spark
spark.logConf=true
spark.master=local[8]
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-3281211262966016233-8fe57c53-b822-4434-871c-c263ce4408f3
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=42605
spark.worker.cleanup.enabled=false
20/02/28 22:47:34 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
20/02/28 22:47:34 INFO log: Logging initialized @7531ms
20/02/28 22:47:34 INFO Server: jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d
20/02/28 22:47:34 INFO Server: Started @7697ms
20/02/28 22:47:34 INFO AbstractConnector: Started ServerConnector@67e25252{HTTP/1.1,[http/1.1]}{10.172.228.50:42605}
20/02/28 22:47:34 INFO Utils: Successfully started service 'SparkUI' on port 42605.
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55fee662{/jobs,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@309cedb6{/jobs/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b95a6db{/jobs/job,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2c9a6717{/jobs/job/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b3cde6f{/stages,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d091cad{/stages/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7c663eaf{/stages/stage,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3ba0ae41{/stages/stage/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@76fe6cdc{/stages/pool,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ffb3aec{/stages/pool/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@786ff1cb{/storage,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@46039a21{/storage/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@431e86b1{/storage/rdd,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35c4e864{/storage/rdd/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32a2a6be{/environment,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@682af059{/environment/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f36c8e3{/executors,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4da39ca9{/executors/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6a9344f5{/executors/threadDump,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5584d9c6{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3c9c6245{/executors/heapHistogram,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d0be7ab{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d4fb213{/static,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1668919e{/,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@63300c4b{/api,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@b67cc70{/jobs/job/kill,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@45c9b3{/stages/stage/kill,null,AVAILABLE,@Spark}
20/02/28 22:47:34 INFO SparkUI: Bound SparkUI to 10.172.228.50, and started at http://10.172.228.50:42605
20/02/28 22:47:34 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
20/02/28 22:47:34 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
20/02/28 22:47:34 INFO Executor: Starting executor ID driver on host localhost
20/02/28 22:47:35 INFO Executor: Using REPL class URI: spark://10.172.228.50:34230/classes
20/02/28 22:47:35 INFO TaskSchedulerImpl: Task preemption enabled.
20/02/28 22:47:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33830.
20/02/28 22:47:35 INFO NettyBlockTransferService: Server created on 10.172.228.50:33830
20/02/28 22:47:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/02/28 22:47:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.172.228.50, 33830, None)
20/02/28 22:47:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.172.228.50:33830 with 2.5 GB RAM, BlockManagerId(driver, 10.172.228.50, 33830, None)
20/02/28 22:47:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.172.228.50, 33830, None)
20/02/28 22:47:35 INFO BlockManager: external shuffle service port = 4048
20/02/28 22:47:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.172.228.50, 33830, None)
20/02/28 22:47:35 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1039bfc4{/metrics/json,null,AVAILABLE,@Spark}
20/02/28 22:47:35 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
20/02/28 22:47:35 INFO DBCEventLoggingListener: Logging events to eventlogs/3281211262966016233/eventlog
20/02/28 22:47:35 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
20/02/28 22:47:35 INFO SparkContext: Loading Spark Service RPC Server
20/02/28 22:47:36 INFO SparkServiceRPCServer: Starting Spark Service RPC Server
20/02/28 22:47:36 INFO Server: jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d
20/02/28 22:47:36 INFO AbstractConnector: Started ServerConnector@543d5863{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}
20/02/28 22:47:36 INFO Server: Started @9183ms
20/02/28 22:47:36 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
20/02/28 22:47:36 INFO DatabricksILoop$: Successfully initialized SparkContext
20/02/28 22:47:36 INFO DBFS: Initialized DBFS with DBFSV1 as the delegate.
20/02/28 22:47:36 WARN JettyClientConf$: Could not load DynamicJettyClientConf
java.lang.ClassNotFoundException: com.databricks.featureflag.client.DynamicJettyClientConf$
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at com.databricks.rpc.JettyClientConf$.liftedTree1$1(BaseJettyClient.scala:1013)
	at com.databricks.rpc.JettyClientConf$.conf$lzycompute(BaseJettyClient.scala:1007)
	at com.databricks.rpc.JettyClientConf$.conf(BaseJettyClient.scala:1006)
	at com.databricks.rpc.DynamicJettyClient.client(BaseJettyClient.scala:536)
	at com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:553)
	at com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:36)
	at com.databricks.rpc.PeerHealthTracker.liftedTree1$1(PeerHealth.scala:65)
	at com.databricks.rpc.PeerHealthTracker.get(PeerHealth.scala:61)
	at com.databricks.rpc.ReliableJettyClient.ensureUp(ReliableJettyClient.scala:137)
	at com.databricks.rpc.ReliableJettyClient.sendIdempotent(ReliableJettyClient.scala:62)
	at com.databricks.backend.daemon.data.client.DbfsClient$$anonfun$doSend$2.apply(DbfsClient.scala:167)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:239)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:234)
	at com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:23)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:276)
	at com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:23)
	at com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:147)
	at com.databricks.backend.daemon.data.client.DbfsClient.sessionId$lzycompute(DbfsClient.scala:26)
	at com.databricks.backend.daemon.data.client.DbfsClient.sessionId(DbfsClient.scala:26)
	at com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:100)
	at com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:63)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.delete(DatabricksFileSystemV1.scala:153)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.delete(DatabricksFileSystem.scala:138)
	at com.databricks.backend.daemon.driver.DatabricksILoop$.deleteHiveDirectories(DatabricksILoop.scala:521)
	at com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:374)
	at com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:274)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
	at com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:212)
	at com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
	at com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
	at com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
	at com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:92)
	at com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:91)
	at com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:91)
	at com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:287)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:417)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:239)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:234)
	at com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:61)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:276)
	at com.databricks.DatabricksMain.withAttributionTags(DatabricksMain.scala:61)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:398)
	at com.databricks.DatabricksMain.recordOperation(DatabricksMain.scala:61)
	at com.databricks.DatabricksMain.withStartupProfilingData(DatabricksMain.scala:286)
	at com.databricks.DatabricksMain.main(DatabricksMain.scala:91)
	at com.databricks.backend.daemon.driver.DriverDaemon.main(DriverDaemon.scala)
20/02/28 22:47:36 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DBFSV1), idleTimeout=2 hours, useBlockingConnect: true
20/02/28 22:47:37 INFO SharedState: Scheduler stats enabled.
20/02/28 22:47:37 INFO SharedState: loading hive config file: file:/databricks/hive/conf/hive-site.xml
20/02/28 22:47:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
20/02/28 22:47:37 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
20/02/28 22:47:37 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5b066c33{/SQL,null,AVAILABLE,@Spark}
20/02/28 22:47:37 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@62ea8931{/SQL/json,null,AVAILABLE,@Spark}
20/02/28 22:47:37 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@71166348{/SQL/execution,null,AVAILABLE,@Spark}
20/02/28 22:47:37 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d874695{/SQL/execution/json,null,AVAILABLE,@Spark}
20/02/28 22:47:37 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10ed037a{/static/sql,null,AVAILABLE,@Spark}
20/02/28 22:47:37 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@150fc7a7{/storage/iocache,null,AVAILABLE,@Spark}
20/02/28 22:47:37 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55d8c2c4{/storage/iocache/json,null,AVAILABLE,@Spark}
20/02/28 22:47:37 INFO DatabricksILoop$: Finished creating throwaway interpreter
20/02/28 22:47:38 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/02/28 22:47:40 INFO HiveUtils: Initializing execution hive, version 1.2.1
20/02/28 22:47:41 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/02/28 22:47:41 INFO ObjectStore: ObjectStore, initialize called
20/02/28 22:47:41 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/02/28 22:47:41 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/02/28 22:47:41 INFO Persistence: Property datanucleus.schema.autoCreateAll unknown - will be ignored
20/02/28 22:47:44 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/02/28 22:47:46 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:47:46 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:47:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:47:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:47:47 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/02/28 22:47:47 INFO ObjectStore: Initialized ObjectStore
20/02/28 22:47:47 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/02/28 22:47:48 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/02/28 22:47:48 INFO HiveMetaStore: Added admin role in metastore
20/02/28 22:47:48 INFO HiveMetaStore: Added public role in metastore
20/02/28 22:47:48 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/02/28 22:47:48 INFO HiveMetaStore: 0: get_all_databases
20/02/28 22:47:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
20/02/28 22:47:48 INFO HiveMetaStore: 0: get_functions: db=default pat=*
20/02/28 22:47:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20/02/28 22:47:48 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:47:48 INFO SessionState: Created HDFS directory: /tmp/hive/root
20/02/28 22:47:48 INFO SessionState: Created local directory: /local_disk0/tmp/root
20/02/28 22:47:48 INFO SessionState: Created local directory: /local_disk0/tmp/efd228cc-d90c-43d8-a496-232484c2a51c_resources
20/02/28 22:47:49 INFO SessionState: Created HDFS directory: /tmp/hive/root/efd228cc-d90c-43d8-a496-232484c2a51c
20/02/28 22:47:49 INFO SessionState: Created local directory: /local_disk0/tmp/root/efd228cc-d90c-43d8-a496-232484c2a51c
20/02/28 22:47:49 INFO SessionState: Created HDFS directory: /tmp/hive/root/efd228cc-d90c-43d8-a496-232484c2a51c/_tmp_space.db
20/02/28 22:47:49 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
20/02/28 22:47:49 INFO SessionManager: Operation log root directory is created: /local_disk0/tmp/root/operation_logs
20/02/28 22:47:49 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
20/02/28 22:47:49 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
20/02/28 22:47:49 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
20/02/28 22:47:49 INFO AbstractService: Service:OperationManager is inited.
20/02/28 22:47:49 INFO AbstractService: Service:SessionManager is inited.
20/02/28 22:47:49 INFO AbstractService: Service: CLIService is inited.
20/02/28 22:47:49 INFO AbstractService: Service:ThriftHttpCLIService is inited.
20/02/28 22:47:49 INFO AbstractService: Service: HiveServer2 is inited.
20/02/28 22:47:49 INFO AbstractService: Service:OperationManager is started.
20/02/28 22:47:49 INFO AbstractService: Service:SessionManager is started.
20/02/28 22:47:49 INFO AbstractService: Service:CLIService is started.
20/02/28 22:47:49 INFO ObjectStore: ObjectStore, initialize called
20/02/28 22:47:49 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20/02/28 22:47:49 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/02/28 22:47:49 INFO ObjectStore: Initialized ObjectStore
20/02/28 22:47:49 INFO HiveMetaStore: 0: get_databases: default
20/02/28 22:47:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
20/02/28 22:47:49 INFO HiveMetaStore: 0: Shutting down the object store...
20/02/28 22:47:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
20/02/28 22:47:49 INFO HiveMetaStore: 0: Metastore shutdown complete.
20/02/28 22:47:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
20/02/28 22:47:49 INFO AbstractService: Service:ThriftHttpCLIService is started.
20/02/28 22:47:49 INFO AbstractService: Service:HiveServer2 is started.
20/02/28 22:47:49 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
20/02/28 22:47:49 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
20/02/28 22:47:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@263f6e96{/sqlserver,null,AVAILABLE,@Spark}
20/02/28 22:47:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49741e80{/sqlserver/json,null,AVAILABLE,@Spark}
20/02/28 22:47:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@dd3e1e3{/sqlserver/session,null,AVAILABLE,@Spark}
20/02/28 22:47:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2686a801{/sqlserver/session/json,null,AVAILABLE,@Spark}
20/02/28 22:47:49 WARN LibraryUtils$: Library file for validation /databricks/.python-env/packages_to_validate.json does not exist
20/02/28 22:47:49 INFO DriverDaemon: Starting driver daemon...
20/02/28 22:47:49 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
20/02/28 22:47:49 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
20/02/28 22:47:49 INFO DriverDaemon$$anon$1: Message out thread ready
20/02/28 22:47:49 INFO Server: jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d
20/02/28 22:47:49 INFO AbstractConnector: Started ServerConnector@1a34f8e2{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}
20/02/28 22:47:49 INFO Server: Started @22781ms
20/02/28 22:47:49 INFO DriverDaemon: Driver daemon started.
20/02/28 22:47:49 INFO Server: jetty-9.3.27.v20190418, build timestamp: 2019-04-18T18:11:38Z, git hash: d3e249f86955d04bc646bb620905b7c1bc596a8d
20/02/28 22:47:49 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@77d00969{/,null,STARTING} has uncovered http methods for path: /*
20/02/28 22:47:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@77d00969{/,null,AVAILABLE}
20/02/28 22:47:49 INFO SslContextFactory: x509=X509@2f07e871(1,h=[databrickscloud.com],w=[]) for SslContextFactory@711ca071(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)
20/02/28 22:47:49 INFO AbstractConnector: Started ServerConnector@3d2b92b5{SSL,[ssl, http/1.1]}{0.0.0.0:10000}
20/02/28 22:47:49 INFO Server: Started @22881ms
20/02/28 22:47:49 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
20/02/28 22:47:51 INFO DriverCorral: Loading the root classloader
20/02/28 22:47:51 INFO DriverCorral: Starting sql repl ReplId-26113-21226-c29bd-0
20/02/28 22:47:51 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:51 INFO DriverCorral: Starting sql repl ReplId-6fffc-24c49-c4ef9-6
20/02/28 22:47:51 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:51 INFO SQLDriverWrapper: setupRepl:ReplId-26113-21226-c29bd-0: finished to load
20/02/28 22:47:51 INFO SQLDriverWrapper: setupRepl:ReplId-6fffc-24c49-c4ef9-6: finished to load
20/02/28 22:47:51 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DriverDaemon), idleTimeout=2 hours, useBlockingConnect: true
20/02/28 22:47:51 INFO DriverCorral: Starting sql repl ReplId-7f7de-3f667-b27c3-1
20/02/28 22:47:51 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:51 INFO SQLDriverWrapper: setupRepl:ReplId-7f7de-3f667-b27c3-1: finished to load
20/02/28 22:47:51 INFO DriverCorral: Starting sql repl ReplId-7b683-976ad-dfc7c-f
20/02/28 22:47:51 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:51 INFO SQLDriverWrapper: setupRepl:ReplId-7b683-976ad-dfc7c-f: finished to load
20/02/28 22:47:51 INFO DriverCorral: Starting sql repl ReplId-19711-fca89-c26d6
20/02/28 22:47:51 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:51 INFO SQLDriverWrapper: setupRepl:ReplId-19711-fca89-c26d6: finished to load
20/02/28 22:47:51 INFO DriverCorral: Starting r repl ReplId-3593d-ebddd-8b23a-e
20/02/28 22:47:51 WARN RDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:51 INFO RDriverLocal: 1. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: object created with for ReplId-3593d-ebddd-8b23a-e.
20/02/28 22:47:51 INFO RDriverLocal: 2. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: initializing ...
20/02/28 22:47:52 INFO RDriverLocal: 3. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: started RBackend thread on port 41274
20/02/28 22:47:52 INFO RDriverLocal: 4. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: waiting for SparkR to be installed ...
20/02/28 22:47:52 INFO DriverCorral: Starting python repl ReplId-8497b-f4c2d-af4d9
20/02/28 22:47:52 WARN PythonDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:52 INFO PythonDriverLocal: Starting gateway server for repl ReplId-8497b-f4c2d-af4d9
20/02/28 22:47:52 INFO Utils: resolved command to be run: List(virtualenv, /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b, --no-site-packages, -p, /databricks/python/bin/python, --no-setuptools, --no-wheel, --no-pip)
20/02/28 22:47:52 INFO DriverCorral: Starting scala repl ReplId-641e1-43796-385e1-8
20/02/28 22:47:52 WARN ScalaDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:52 INFO ScalaDriverWrapper: setupRepl:ReplId-641e1-43796-385e1-8: finished to load
20/02/28 22:47:52 INFO DriverCorral: Starting sql repl ReplId-4703f-a6518-7dd28-9
20/02/28 22:47:52 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:47:52 INFO SQLDriverWrapper: setupRepl:ReplId-4703f-a6518-7dd28-9: finished to load
20/02/28 22:47:53 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_8003361428351860078_972c5bd7-c187-4c99-ab26-32425d02c2dc
20/02/28 22:47:53 INFO ProgressReporter$: Added result fetcher for 7214225882070146584_6622863124546160429_3bab45f008c44d3b8a54b200eb043826
20/02/28 22:47:53 INFO DatabricksUtils: created python virtualenv: /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b
20/02/28 22:47:53 INFO Utils: resolved command to be run: List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/bin/python, -c, from distutils.sysconfig import get_python_lib; print(get_python_lib()))
20/02/28 22:47:53 INFO SignalUtils: Registered signal handler for INT
20/02/28 22:47:53 INFO Utils: resolved command to be run: List(/databricks/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))
20/02/28 22:47:53 INFO DatabricksUtils: created sitecustomize.py at /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/lib/python3.7/sitecustomize.py
20/02/28 22:47:53 INFO PythonDriverLocal$: Time spent to start virtualenv /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b is 1090
20/02/28 22:47:53 INFO PythonDriverLocal$: Python process builder: [/databricks/spark/python/pyspark/wrapped_python.py, root, /local_disk0/pythonVirtualEnvDirs/virtualEnv-ff80c1c3-b8f2-4aa9-a3d2-992aedee6f5b/bin/python, -u, /local_disk0/tmp/1582930072342-0/PythonShell.py, 43339, 0, 50000, 2177, 218164f3f65d4a4ca63276a38e458e45, 2.4.4, 0bece102c3a57c7e446912d4a03cdd4feaf1aacc0a97cdf63fb5412c44f02820]
20/02/28 22:47:53 INFO PythonDriverLocal$: Cgroup isolation disabled, not placing python process in repl cgroup
20/02/28 22:47:54 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
20/02/28 22:47:55 INFO DriverILoop: Set class prefix to: line49b319df9593406199cec71f0268458a
20/02/28 22:47:55 INFO DriverILoop: set ContextClassLoader
20/02/28 22:47:55 INFO DriverILoop: initialized intp
20/02/28 22:47:56 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/daemon--data--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:file:/databricks/hive/third_party--jackson--guava_only_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/databricks/hive/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:file:/databricks/hive/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:file:/databricks/hive/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/databricks/hive/s3commit--common--common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/databricks/hive/third_party--jackson--paranamer_only_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/common--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-storage__7.0.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/databricks/hive/third_party--jetty-client--jetty-util_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:file:/databricks/hive/----jackson_databind_shaded--libjackson-databind.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.42.Final.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:file:/databricks/hive/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:file:/databricks/hive/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:file:/databricks/hive/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:file:/databricks/hive/third_party--jetty-client--jetty-io_shaded.jar:file:/databricks/hive/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/s3commit--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/hive/common--credentials--credentials-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:file:/databricks/hive/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-jetty9-hadoop1_2.11_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:file:/databricks/hive/extern--extern-spark_2.4_2.11_deploy.jar:file:/databricks/hive/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:file:/databricks/hive/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/hive/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar:file:/databricks/hive/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:file:/databricks/hive/third_party--jetty-client--jetty-client_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/databricks/hive/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/databricks/hive/third_party--jetty-client--jetty-http_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:file:/databricks/hive/s3--s3-spark_2.4_2.11_deploy.jar:file:/databricks/hive/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:file:/databricks/hive/----jackson_annotations_shaded--libjackson-annotations.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:file:/databricks/hive/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:file:/databricks/hive/common--path--path-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/databricks/hive/common--lazy--lazy-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:file:/databricks/hive/api-base--api-base-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/databricks/hive/api-base--api-base_java-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/databricks/hive/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--jackson--jsr305_only_shaded.jar:file:/databricks/hive/extern--libaws-regions.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/databricks/hive/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/databricks/hive/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:file:/databricks/hive/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/databricks/hive/common--jetty--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/----jackson_core_shaded--libjackson-core.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/bonecp-configs.jar
20/02/28 22:47:57 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(0,show databases,org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)
com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.immutable.List.foreach(List.scala:392)
scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
scala.collection.immutable.List.map(List.scala:296)
com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)
com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)
com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:386)
com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:363)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:239)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:234)
com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:276)
com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:363)
com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644),== Parsed Logical Plan ==
ShowDatabasesCommand

== Analyzed Logical Plan ==
databaseName: string
ShowDatabasesCommand

== Optimized Logical Plan ==
ShowDatabasesCommand

== Physical Plan ==
Execute ShowDatabasesCommand
   +- ShowDatabasesCommand,org.apache.spark.sql.execution.SparkPlanInfo@ff6e36fb,1582930076491) by listener DBCEventLoggingListener took 1.066212683s.
20/02/28 22:47:57 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/02/28 22:47:57 INFO ObjectStore: ObjectStore, initialize called
20/02/28 22:47:58 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/02/28 22:47:58 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/02/28 22:47:59 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/02/28 22:47:59 INFO PythonDriverLocal: PythonDriver[ReplId-8497b-f4c2d-af4d9](stateStr) Set Python Interp in JVM for ReplId-8497b-f4c2d-af4d9
20/02/28 22:47:59 INFO PythonDriverWrapper: setupRepl:ReplId-8497b-f4c2d-af4d9: finished to load
20/02/28 22:48:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:48:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:48:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:48:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/02/28 22:48:01 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20/02/28 22:48:01 INFO ObjectStore: Initialized ObjectStore
20/02/28 22:48:01 INFO HiveMetaStore: Added admin role in metastore
20/02/28 22:48:01 INFO HiveMetaStore: Added public role in metastore
20/02/28 22:48:01 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/02/28 22:48:02 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
20/02/28 22:48:02 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is /user/hive/warehouse
20/02/28 22:48:02 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:48:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:48:02 INFO HiveMetaStore: 0: get_databases: *
20/02/28 22:48:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: *	
20/02/28 22:48:04 INFO AsyncEventQueue: Process of event SparkListenerSQLUsageLogging(0,1582930082300,CallSite(sql at SQLDriverLocal.scala:88,org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)
com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.immutable.List.foreach(List.scala:392)
scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
scala.collection.immutable.List.map(List.scala:296)
com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)
com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)
com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:386)
com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:363)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:239)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:234)
com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:276)
com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:363)
com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)),org.apache.spark.sql.internal.SQLConf@3e65f406,Some(CommandContext(Map(opId -> ServerBackend-291b4a74dae70078, opTarget -> com.databricks.backend.common.rpc.InternalDriverBackendMessages$StartRepl, serverBackendName -> com.databricks.backend.daemon.driver.DriverCorral, notebookId -> 3922321368675345, projectName -> chauffeur, eventWindowTime -> 83187.03000014648, httpTarget -> /websocket, buildHash -> 687e04cafbfe88e512c6aea9e4f53a2c4e1d5689, webSocketRpcMethod -> query, browserHash -> #notebook/3922321368675345/command/3922321368675362, host -> 10.172.228.50, notebookLanguage -> python, hostName -> 0228-224707-rangy181-10-172-228-50, httpMethod -> GET, browserIdleTime -> 45461, jettyRpcJettyVersion -> 8, browserTabId -> 4278631c-4fa1-4092-9a7c-4f4122aa53b2, sourceIpAddress -> 90.22.173.139, browserUserAgent -> Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36, orgId -> 823618267676840, userAgent -> Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36, rootOpId -> ServiceMain-2487d2344a250002, sessionId -> ephemeral-20564474-a5f6-45a3-b1ca-28525bc7fb45, clientBranchName -> 3.14.2023, browserHasFocus -> true, userId -> 5472156576229446, browserIsHidden -> false, opType -> ServerBackend, sourcePortNumber -> 54630, user -> w.yunbo@gmail.com, browserHostName -> community.cloud.databricks.com, parentOpId -> RPCClient-2487d2344a2500bc, jettyRpcType -> InternalDriverBackendMessages$DriverBackendRequest),Map(api_url -> https://community.cloud.databricks.com, api_token -> [REDACTED]))),== Parsed Logical Plan ==
ShowDatabasesCommand

== Analyzed Logical Plan ==
databaseName: string
ShowDatabasesCommand

== Optimized Logical Plan ==
ShowDatabasesCommand

== Physical Plan ==
Execute ShowDatabasesCommand
   +- ShowDatabasesCommand,None,None,None) by listener SQLAppStatusListener took 1.85076385s.
20/02/28 22:48:04 INFO SQLAppStatusListener: Execution ID: 0 Total Executor Run Time: 0
20/02/28 22:48:04 INFO SQLAppStatusListener: Execution ID: 1 Total Executor Run Time: 0
20/02/28 22:48:04 INFO CodeGenerator: Code generated in 803.712518 ms
20/02/28 22:48:04 INFO CodeGenerator: Code generated in 64.586368 ms
20/02/28 22:48:04 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_8003361428351860078_972c5bd7-c187-4c99-ab26-32425d02c2dc
20/02/28 22:48:05 INFO SQLAppStatusListener: Execution ID: 2 Total Executor Run Time: 0
20/02/28 22:48:05 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_6510401489672478420_9e0d4829-5908-405b-915b-df5149c76d2c
20/02/28 22:48:06 INFO HiveMetaStore: 0: get_database: global_temp
20/02/28 22:48:06 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/02/28 22:48:06 ERROR RetryingHMSHandler: NoSuchObjectException(message:There is no database named global_temp)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:487)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:498)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy44.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy45.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:949)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy46.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1165)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1154)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply$mcZ$sp(HiveClientImpl.scala:412)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:412)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:412)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:331)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$retryLocked$1.apply(HiveClientImpl.scala:239)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$retryLocked$1.apply(HiveClientImpl.scala:231)
	at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:231)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:314)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:411)
	at org.apache.spark.sql.hive.client.PoolingHiveClient$$anonfun$databaseExists$1.apply(PoolingHiveClient.scala:267)
	at org.apache.spark.sql.hive.client.PoolingHiveClient$$anonfun$databaseExists$1.apply(PoolingHiveClient.scala:266)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.withHiveClient(PoolingHiveClient.scala:112)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.databaseExists(PoolingHiveClient.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:279)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:279)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:279)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:278)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.databaseExists(ExternalCatalogWithListener.scala:74)
	at org.apache.spark.sql.internal.SharedState$$anonfun$1.apply$mcZ$sp(SharedState.scala:183)
	at org.apache.spark.sql.internal.SharedState$$anonfun$1.apply(SharedState.scala:183)
	at org.apache.spark.sql.internal.SharedState$$anonfun$1.apply(SharedState.scala:183)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:183)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:177)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:59)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:59)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:789)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:779)
	at org.apache.spark.sql.execution.command.ShowTablesCommand$$anonfun$17.apply(tables.scala:779)
	at org.apache.spark.sql.execution.command.ShowTablesCommand$$anonfun$17.apply(tables.scala:779)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:779)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3485)
	at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3480)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3480)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
	at com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)
	at com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:386)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:363)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:239)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:234)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:276)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:363)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
	at java.lang.Thread.run(Thread.java:748)

20/02/28 22:48:06 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:48:06 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:48:06 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:48:06 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:48:06 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/02/28 22:48:06 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/02/28 22:48:06 INFO SQLAppStatusListener: Execution ID: 3 Total Executor Run Time: 0
20/02/28 22:48:06 INFO SQLAppStatusListener: Execution ID: 4 Total Executor Run Time: 0
20/02/28 22:48:06 INFO CodeGenerator: Code generated in 23.73242 ms
20/02/28 22:48:06 INFO SQLAppStatusListener: Execution ID: 5 Total Executor Run Time: 0
20/02/28 22:48:06 INFO CodeGenerator: Code generated in 21.351697 ms
20/02/28 22:48:06 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_6510401489672478420_9e0d4829-5908-405b-915b-df5149c76d2c
20/02/28 22:48:06 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_8863609662602837032_4cdd00ca-2f61-4026-8edc-588d2c885897
20/02/28 22:48:07 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:48:07 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:48:07 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_final
20/02/28 22:48:07 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final	
20/02/28 22:48:07 INFO ProgressReporter$: Removed result fetcher for 7214225882070146584_6622863124546160429_3bab45f008c44d3b8a54b200eb043826
20/02/28 22:48:07 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_final
20/02/28 22:48:07 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final	
20/02/28 22:48:07 INFO SQLAppStatusListener: Execution ID: 6 Total Executor Run Time: 0
20/02/28 22:48:07 INFO SQLAppStatusListener: Execution ID: 7 Total Executor Run Time: 0
20/02/28 22:48:08 INFO CodeGenerator: Code generated in 34.800637 ms
20/02/28 22:48:08 INFO SQLAppStatusListener: Execution ID: 8 Total Executor Run Time: 0
20/02/28 22:48:08 INFO CodeGenerator: Code generated in 23.702847 ms
20/02/28 22:48:08 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_8863609662602837032_4cdd00ca-2f61-4026-8edc-588d2c885897
20/02/28 22:48:08 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_7301126850103711220_1c069279-6fad-481c-97db-f2a621e86f7f
20/02/28 22:48:08 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:48:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:48:08 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_20200215
20/02/28 22:48:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215	
20/02/28 22:48:08 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_20200215
20/02/28 22:48:08 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215	
20/02/28 22:48:09 INFO SQLAppStatusListener: Execution ID: 9 Total Executor Run Time: 0
20/02/28 22:48:09 INFO SQLAppStatusListener: Execution ID: 10 Total Executor Run Time: 0
20/02/28 22:48:09 INFO SQLAppStatusListener: Execution ID: 11 Total Executor Run Time: 0
20/02/28 22:48:09 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_7301126850103711220_1c069279-6fad-481c-97db-f2a621e86f7f
20/02/28 22:48:17 INFO RDriverLocal$: SparkR installation completed.
20/02/28 22:48:17 INFO RDriverLocal: 5. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: launching R process ...
20/02/28 22:48:17 INFO RDriverLocal: 6. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: cgroup isolation disabled, not placing R process in REPL cgroup.
20/02/28 22:48:17 INFO RDriverLocal: 7. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: starting R process on port 1100 (attempt 1) ...
20/02/28 22:48:17 INFO RDriverLocal: 8. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: setting up BufferedStreamThread with bufferSize: 100.
20/02/28 22:48:18 INFO RDriverLocal: 9. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: R process started with RServe listening on port 1100.
20/02/28 22:48:19 INFO RDriverLocal: 10. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: starting interpreter to talk to R process ...
20/02/28 22:48:20 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
20/02/28 22:48:20 INFO RDriverLocal: 11. RDriverLocal.b6b4775f-de01-4706-97e6-5853537007e5: R interpretter is connected.
20/02/28 22:48:20 INFO RDriverWrapper: setupRepl:ReplId-3593d-ebddd-8b23a-e: finished to load
20/02/28 22:50:09 INFO ContextCleaner: Cleaned accumulator 2 (name: number of output rows)
20/02/28 22:50:09 INFO ContextCleaner: Cleaned accumulator 3 (name: number of output rows)
20/02/28 22:50:09 INFO ContextCleaner: Cleaned accumulator 4 (name: number of output rows)
20/02/28 22:50:09 INFO ContextCleaner: Cleaned accumulator 1 (name: number of output rows)
20/02/28 22:52:31 INFO DriverCorral: DBFS health check ok
20/02/28 22:52:32 INFO HikariDataSource: metastore-monitor - Starting...
20/02/28 22:52:32 INFO HikariDataSource: metastore-monitor - Start completed.
20/02/28 22:52:32 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
20/02/28 22:52:32 INFO HikariDataSource: metastore-monitor - Shutdown completed.
20/02/28 22:52:32 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 20 milliseconds)
20/02/28 22:52:49 INFO HiveMetaStore: 1: get_database: default
20/02/28 22:52:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:52:49 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/02/28 22:52:49 INFO ObjectStore: ObjectStore, initialize called
20/02/28 22:52:49 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20/02/28 22:52:49 INFO ObjectStore: Initialized ObjectStore
20/02/28 22:52:49 INFO DriverCorral: Metastore health check ok
20/02/28 22:53:11 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_8575408154924242561_1aabdc66-d2b7-488e-8627-95614094595d
20/02/28 22:53:11 INFO HiveMetaStore: 0: get_databases: *
20/02/28 22:53:11 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: *	
20/02/28 22:53:11 INFO SQLAppStatusListener: Execution ID: 12 Total Executor Run Time: 0
20/02/28 22:53:11 INFO SQLAppStatusListener: Execution ID: 13 Total Executor Run Time: 0
20/02/28 22:53:11 INFO CodeGenerator: Code generated in 17.110219 ms
20/02/28 22:53:11 INFO SQLAppStatusListener: Execution ID: 14 Total Executor Run Time: 0
20/02/28 22:53:11 INFO CodeGenerator: Code generated in 15.345658 ms
20/02/28 22:53:11 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_8575408154924242561_1aabdc66-d2b7-488e-8627-95614094595d
20/02/28 22:53:12 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_5622342512599412368_13f4066b-a605-4bf5-812a-05779a9cd787
20/02/28 22:53:12 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:53:12 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:53:12 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:53:12 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:53:12 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/02/28 22:53:12 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/02/28 22:53:12 INFO SQLAppStatusListener: Execution ID: 15 Total Executor Run Time: 0
20/02/28 22:53:12 INFO SQLAppStatusListener: Execution ID: 16 Total Executor Run Time: 0
20/02/28 22:53:12 INFO CodeGenerator: Code generated in 22.147679 ms
20/02/28 22:53:12 INFO SQLAppStatusListener: Execution ID: 17 Total Executor Run Time: 0
20/02/28 22:53:12 INFO CodeGenerator: Code generated in 28.274594 ms
20/02/28 22:53:12 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_5622342512599412368_13f4066b-a605-4bf5-812a-05779a9cd787
20/02/28 22:53:13 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_8172190809674484735_a05caece-40ae-41b5-8b9a-2680835f69cb
20/02/28 22:53:13 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:53:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:53:13 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_final
20/02/28 22:53:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final	
20/02/28 22:53:13 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_final
20/02/28 22:53:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_final	
20/02/28 22:53:13 INFO SQLAppStatusListener: Execution ID: 18 Total Executor Run Time: 0
20/02/28 22:53:13 INFO SQLAppStatusListener: Execution ID: 19 Total Executor Run Time: 0
20/02/28 22:53:14 INFO CodeGenerator: Code generated in 24.176687 ms
20/02/28 22:53:14 INFO SQLAppStatusListener: Execution ID: 20 Total Executor Run Time: 0
20/02/28 22:53:14 INFO CodeGenerator: Code generated in 30.949684 ms
20/02/28 22:53:14 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_8172190809674484735_a05caece-40ae-41b5-8b9a-2680835f69cb
20/02/28 22:53:17 INFO ProgressReporter$: Added result fetcher for 5117208913732555401_7722394248052958781_305af185-7f94-4ba9-a13b-1be60e8ede20
20/02/28 22:53:17 INFO HiveMetaStore: 0: get_database: default
20/02/28 22:53:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:53:17 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_20200215
20/02/28 22:53:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215	
20/02/28 22:53:17 INFO HiveMetaStore: 0: get_table : db=default tbl=visits_20200215
20/02/28 22:53:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=visits_20200215	
20/02/28 22:53:17 INFO SQLAppStatusListener: Execution ID: 21 Total Executor Run Time: 0
20/02/28 22:53:17 INFO SQLAppStatusListener: Execution ID: 22 Total Executor Run Time: 0
20/02/28 22:53:17 INFO SQLAppStatusListener: Execution ID: 23 Total Executor Run Time: 0
20/02/28 22:53:17 INFO ProgressReporter$: Removed result fetcher for 5117208913732555401_7722394248052958781_305af185-7f94-4ba9-a13b-1be60e8ede20
20/02/28 22:53:18 INFO DriverCorral: Starting sql repl ReplId-6b5a9-7ed4d-b78b2-b
20/02/28 22:53:18 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
20/02/28 22:53:18 INFO SQLDriverWrapper: setupRepl:ReplId-6b5a9-7ed4d-b78b2-b: finished to load
20/02/28 22:53:18 INFO ProgressReporter$: Added result fetcher for 597144716774667481_6780863106840874462_1b0a48ee728b41b49255c04182eca8a0
20/02/28 22:57:32 INFO DriverCorral: DBFS health check ok
20/02/28 22:57:32 INFO HikariDataSource: metastore-monitor - Starting...
20/02/28 22:57:32 INFO HikariDataSource: metastore-monitor - Start completed.
20/02/28 22:57:32 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
20/02/28 22:57:32 INFO HikariDataSource: metastore-monitor - Shutdown completed.
20/02/28 22:57:32 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 30 milliseconds)
20/02/28 22:57:49 INFO HiveMetaStore: 1: get_database: default
20/02/28 22:57:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
20/02/28 22:57:49 INFO DriverCorral: Metastore health check ok
20/02/28 22:58:46 INFO ProgressReporter$: Removed result fetcher for 597144716774667481_6780863106840874462_1b0a48ee728b41b49255c04182eca8a0
20/02/28 22:58:46 INFO ProgressReporter$: Added result fetcher for 7214225882070146584_8096105091980147949_1b0a48ee728b41b49255c04182eca8a0
20/02/28 22:58:46 INFO ProgressReporter$: Removed result fetcher for 7214225882070146584_8096105091980147949_1b0a48ee728b41b49255c04182eca8a0
20/02/28 22:58:46 INFO ProgressReporter$: Added result fetcher for 7214225882070146584_8489408173556252300_1b0a48ee728b41b49255c04182eca8a0
20/02/28 22:59:19 INFO DbfsOutputStream: DbfsOutputStream closed, reporting metrics.
20/02/28 22:59:19 INFO ProgressReporter$: Removed result fetcher for 7214225882070146584_8489408173556252300_1b0a48ee728b41b49255c04182eca8a0
20/02/28 22:59:19 INFO ProgressReporter$: Added result fetcher for 7214225882070146584_6575950616243125849_1b0a48ee728b41b49255c04182eca8a0
20/02/28 22:59:32 INFO ContextCleaner: Cleaned accumulator 5 (name: number of output rows)
20/02/28 22:59:32 INFO ContextCleaner: Cleaned accumulator 8 (name: number of output rows)
20/02/28 22:59:32 INFO ContextCleaner: Cleaned accumulator 6 (name: number of output rows)
20/02/28 22:59:32 INFO ContextCleaner: Cleaned accumulator 7 (name: number of output rows)
